{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "# 数据集处理\n",
    "# imdb\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import os\n",
    "import json, csv\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "from openprompt.utils.logging import logger\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "\n",
    "class IMDBProcessor(DataProcessor):\n",
    "    \"\"\"\n",
    "    # TODO citation\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        from openprompt.data_utils.conditional_generation_dataset import PROCESSORS\n",
    "\n",
    "        base_path = \"datasets/CondGen\"\n",
    "\n",
    "        dataset_name = \"webnlg_2017\"\n",
    "        dataset_path = os.path.join(base_path, dataset_name)\n",
    "        processor = PROCESSORS[dataset_name.lower()]()\n",
    "        train_dataset = processor.get_train_examples(dataset_path)\n",
    "        valid_dataset = processor.get_train_examples(dataset_path)\n",
    "        test_dataset = processor.get_test_examples(dataset_path)\n",
    "\n",
    "        assert len(train_dataset) == 18025\n",
    "        assert len(valid_dataset) == 18025\n",
    "        assert len(test_dataset) == 4928\n",
    "        assert test_dataset[0].text_a == \" | Abilene_Regional_Airport : cityServed : Abilene,_Texas\"\n",
    "        assert test_dataset[0].text_b == \"\"\n",
    "        assert test_dataset[0].tgt_text == \"Abilene, Texas is served by the Abilene regional airport.\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_id_only=True, mode=None):\n",
    "        super().__init__()\n",
    "        self.labels = None\n",
    "        self.is_id_only = is_id_only\n",
    "        self.monitor_mode = 'label'\n",
    "\n",
    "    def get_examples(self, data_dir: str, split: str, frac: int = 0) -> List[InputExample]:\n",
    "        examples = []\n",
    "        if split == 'train' and frac:\n",
    "            path = os.path.join(data_dir, \"{}_{}.csv\".format(split, frac))\n",
    "        else:\n",
    "            path = os.path.join(data_dir, \"{}.csv\".format(split))\n",
    "        data = pd.read_csv(path)\n",
    "        intents = list(data[\"intent\"])\n",
    "        # domian_index = list(data['domain_index'])\n",
    "        indexs = list(data[\"index\"])\n",
    "        utts = list(data[\"utt\"])\n",
    "        id_utt = [utts[i] for i in range(len(utts)) if indexs[i] != -1]\n",
    "        # id_intent = [intents[i] for i in range(len(utts)) if indexs[i] != -1]\n",
    "        ood_utt = [utts[i] for i in range(len(utts)) if indexs[i] == -1]\n",
    "        if self.is_id_only:\n",
    "            utts = id_utt\n",
    "            is_oods = [0 for _ in range(len(utts))]\n",
    "            new_indexs = [indexs[i] for i in range(len(indexs)) if indexs[i] != -1]\n",
    "            # new_domian_index = [domian_index[i] for i in range(len(domian_index)) if domian_index[i] != -1]\n",
    "        else:\n",
    "            utts = id_utt + ood_utt\n",
    "            is_oods = [0 for _ in range(len(id_utt))] + [1 for _ in range(len(ood_utt))]\n",
    "            new_indexs = [indexs[i] for i in range(len(indexs)) if indexs[i] != -1] + [-1 for _ in range(len(ood_utt))]\n",
    "            # new_domian_index = [domian_index[i] for i in range(len(domian_index)) if domian_index[i] != -1] + [-1 for _ in range(len(ood_utt))]\n",
    "            assert len(new_indexs) == len(utts)\n",
    "            # assert len(new_domian_index) == len(utts)\n",
    "        if self.monitor_mode == 'label':\n",
    "            monitor = new_indexs\n",
    "        # elif self.monitor_mode == 'domain':\n",
    "        #     monitor = new_domian_index\n",
    "        for i, (tgt, is_ood, intent) in enumerate(zip(utts, is_oods, new_indexs)):\n",
    "            example = InputExample(guid=str(i), text_a=\"\", tgt_text=tgt, meta={\"is_ood\": is_ood},\n",
    "                                   label=intent if is_ood == 0 else -1)  # label=intent  tgt_text=\"the intent is\"\n",
    "            examples.append(example)\n",
    "        return examples\n",
    "\n",
    "    def get_src_tgt_len_ratio(self, ):\n",
    "        pass\n",
    "\n",
    "    def get_label_words(self, data_dir):\n",
    "        path = os.path.join(data_dir, \"train.csv\")\n",
    "        data = pd.read_csv(path)\n",
    "        intents = list(data[\"intent\"])\n",
    "        domian_index = list(data['index'])\n",
    "        result = collections.defaultdict(str)\n",
    "        for intent, index in zip(intents, domian_index):\n",
    "            result[index] = intent\n",
    "        result = sorted(result.items(), key=lambda x: x[0])\n",
    "        result = [[b] for a, b in result]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c195472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "# clinc\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import os\n",
    "import json, csv\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "from openprompt.utils.logging import logger\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "\n",
    "\n",
    "class ClincProcessor(DataProcessor):\n",
    "    \"\"\"\n",
    "    # TODO citation\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        from openprompt.data_utils.conditional_generation_dataset import PROCESSORS\n",
    "\n",
    "        base_path = \"datasets/CondGen\"\n",
    "\n",
    "        dataset_name = \"webnlg_2017\"\n",
    "        dataset_path = os.path.join(base_path, dataset_name)\n",
    "        processor = PROCESSORS[dataset_name.lower()]()\n",
    "        train_dataset = processor.get_train_examples(dataset_path)\n",
    "        valid_dataset = processor.get_train_examples(dataset_path)\n",
    "        test_dataset = processor.get_test_examples(dataset_path)\n",
    "\n",
    "        assert len(train_dataset) == 18025\n",
    "        assert len(valid_dataset) == 18025\n",
    "        assert len(test_dataset) == 4928\n",
    "        assert test_dataset[0].text_a == \" | Abilene_Regional_Airport : cityServed : Abilene,_Texas\"\n",
    "        assert test_dataset[0].text_b == \"\"\n",
    "        assert test_dataset[0].tgt_text == \"Abilene, Texas is served by the Abilene regional airport.\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_id_only=True, mode=None):\n",
    "        super().__init__()\n",
    "        self.labels = None\n",
    "        self.is_id_only = is_id_only\n",
    "        self.monitor_mode = 'label' if mode == 150 else 'domain'\n",
    "\n",
    "    def get_examples(self, data_dir: str, split: str, frac: int = 0) -> List[InputExample]:\n",
    "        examples = []\n",
    "        if split == 'train' and frac:\n",
    "            path = os.path.join(data_dir, \"{}_{}.csv\".format(split, frac))\n",
    "        else:\n",
    "            path = os.path.join(data_dir, \"{}.csv\".format(split))\n",
    "        data = pd.read_csv(path)\n",
    "        intents = list(data[\"intent\"])\n",
    "        domian_index = list(data['domain_index'])\n",
    "        indexs = list(data[\"index\"])\n",
    "        utts = list(data[\"utt\"])\n",
    "        id_utt = [utts[i] for i in range(len(utts)) if indexs[i] != -1]\n",
    "        # id_intent = [intents[i] for i in range(len(utts)) if indexs[i] != -1]\n",
    "        ood_utt = [utts[i] for i in range(len(utts)) if indexs[i] == -1]\n",
    "        if self.is_id_only:\n",
    "            utts = id_utt\n",
    "            is_oods = [0 for _ in range(len(utts))]\n",
    "            new_indexs = [indexs[i] for i in range(len(indexs)) if indexs[i] != -1]\n",
    "            new_domian_index = [domian_index[i] for i in range(len(domian_index)) if domian_index[i] != -1]\n",
    "        else:\n",
    "            utts = id_utt + ood_utt\n",
    "            is_oods = [0 for _ in range(len(id_utt))] + [1 for _ in range(len(ood_utt))]\n",
    "            new_indexs = [indexs[i] for i in range(len(indexs)) if indexs[i] != -1] + [-1 for _ in range(len(ood_utt))]\n",
    "            new_domian_index = [domian_index[i] for i in range(len(domian_index)) if domian_index[i] != -1] + [-1 for _\n",
    "                                                                                                               in range(\n",
    "                    len(ood_utt))]\n",
    "            assert len(new_indexs) == len(utts)\n",
    "            assert len(new_domian_index) == len(utts)\n",
    "        if self.monitor_mode == 'label':\n",
    "            monitor = new_indexs\n",
    "        elif self.monitor_mode == 'domain':\n",
    "            monitor = new_domian_index\n",
    "        for i, (tgt, is_ood, intent) in enumerate(zip(utts, is_oods, monitor)):\n",
    "            example = InputExample(guid=str(i), text_a=\"\", tgt_text=tgt, meta={\"is_ood\": is_ood},\n",
    "                                   label=intent)  # label=intent  tgt_text=\"the intent is\"\n",
    "            examples.append(example)\n",
    "\n",
    "        return examples\n",
    "\n",
    "    def get_src_tgt_len_ratio(self, ):\n",
    "        pass\n",
    "\n",
    "    def get_label_words(self, data_dir):\n",
    "        path = os.path.join(data_dir, \"train.csv\")\n",
    "        data = pd.read_csv(path)\n",
    "        intents = list(data[\"intent\"])\n",
    "        domian_index = list(data['index'])\n",
    "        result = collections.defaultdict(str)\n",
    "        for intent, index in zip(intents, domian_index):\n",
    "            result[index] = intent\n",
    "        result = sorted(result.items(), key=lambda x: x[0])\n",
    "        result = [[b] for a, b in result]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d10457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import yaml\n",
    "def load_parameters_from_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        parameters = yaml.safe_load(file)\n",
    "    return parameters\n",
    "yaml_file_path = 'paras copy.yml'\n",
    "loaded_parameters = load_parameters_from_yaml(yaml_file_path)\n",
    "# print(\"Loaded Parameters:\")\n",
    "# print(loaded_parameters['dataset']['image_size'])\n",
    "config = loaded_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0d0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "if config['dataset'] == 'IMDB':\n",
    "    OOD_DataProcessor = IMDBProcessor\n",
    "    datasets_dir = \"./datasets/imdb_yelp\"\n",
    "    max_seq_length = 256\n",
    "    batch_size = config['batch_size']\n",
    "\n",
    "elif config['dataset'] == 'clinc':\n",
    "    OOD_DataProcessor = ClincProcessor \n",
    "    datasets_dir = \"./datasets/clinc150/\"\n",
    "    max_seq_length = 128\n",
    "    batch_size = config['batch_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "dataset = {}\n",
    "dataset['train'] = OOD_DataProcessor(True).get_examples(datasets_dir, \"train\")\n",
    "# dataset['val'] = OOD_DataProcessor(True).get_examples(datasets_dir, \"valid\")\n",
    "\n",
    "dataset['val'] = OOD_DataProcessor(True).get_examples(datasets_dir, \"test\")\n",
    "\n",
    "dataset[\"val_ood\"] = OOD_DataProcessor(False).get_examples(datasets_dir, \"valid\")\n",
    "dataset['test'] = OOD_DataProcessor(False).get_examples(datasets_dir, \"test\")\n",
    "print(dataset['val'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c949c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "# dataloader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from typing import List, Dict\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, str]], tokenizer: BertTokenizer, max_length: int = 128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"tgt_text\"]\n",
    "        label = self.data[idx][\"label\"]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_data_loader(data: List[Dict[str, str]], tokenizer: BertTokenizer, batch_size: int = 32, max_length: int = 128):\n",
    "    dataset = TextClassificationDataset(data, tokenizer, max_length)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "def create_test_data_loader(data: List[Dict[str, str]], tokenizer: BertTokenizer, batch_size: int = 32, max_length: int = 128):\n",
    "    dataset = TextClassificationDataset(data, tokenizer, max_length)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da095890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "# 评价指标\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def compute_all_metrics(conf, label, pred):\n",
    "    np.set_printoptions(precision=3)\n",
    "    recall = 0.95\n",
    "    auroc, aupr_in, aupr_out, fpr = auc_and_fpr_recall(conf, label, recall)\n",
    "\n",
    "    accuracy = acc(pred, label)\n",
    "\n",
    "    results = [fpr, auroc, aupr_in, aupr_out, accuracy]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# accuracy\n",
    "def acc(pred, label):\n",
    "    ind_pred = pred[label != -1] #id acc\n",
    "    ind_label = label[label != -1]\n",
    "\n",
    "    # ind_pred = pred #all acc\n",
    "    # ind_label = label\n",
    "\n",
    "    num_tp = np.sum(ind_pred == ind_label)\n",
    "    acc = num_tp / len(ind_label)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "# fpr_recall\n",
    "def fpr_recall(conf, label, tpr):\n",
    "    gt = np.ones_like(label)\n",
    "    gt[label == -1] = 0\n",
    "\n",
    "    fpr_list, tpr_list, threshold_list = metrics.roc_curve(gt, conf)\n",
    "    fpr = fpr_list[np.argmax(tpr_list >= tpr)]\n",
    "    thresh = threshold_list[np.argmax(tpr_list >= tpr)]\n",
    "    return fpr, thresh\n",
    "\n",
    "\n",
    "# auc\n",
    "def auc_and_fpr_recall(conf, label, tpr_th):\n",
    "    # following convention in ML we treat OOD as positive\n",
    "    ood_indicator = np.zeros_like(label)\n",
    "    ood_indicator[label == -1] = 1\n",
    "\n",
    "    # in the postprocessor we assume ID samples will have larger\n",
    "    # \"conf\" values than OOD samples\n",
    "    # therefore here we need to negate the \"conf\" values\n",
    "    \n",
    "    fpr_list, tpr_list, thresholds = metrics.roc_curve(ood_indicator, -conf)\n",
    "    fpr = fpr_list[np.argmax(tpr_list >= tpr_th)]\n",
    "\n",
    "    precision_in, recall_in, thresholds_in \\\n",
    "        = metrics.precision_recall_curve(1 - ood_indicator, conf)\n",
    "\n",
    "    precision_out, recall_out, thresholds_out \\\n",
    "        = metrics.precision_recall_curve(ood_indicator, -conf)\n",
    "\n",
    "    auroc = metrics.auc(fpr_list, tpr_list)\n",
    "    aupr_in = metrics.auc(recall_in, precision_in)\n",
    "    aupr_out = metrics.auc(recall_out, precision_out)\n",
    "\n",
    "    return auroc, aupr_in, aupr_out, fpr\n",
    "\n",
    "\n",
    "# ccr_fpr\n",
    "def ccr_fpr(conf, fpr, pred, label):\n",
    "    ind_conf = conf[label != -1]\n",
    "    ind_pred = pred[label != -1]\n",
    "    ind_label = label[label != -1]\n",
    "\n",
    "    ood_conf = conf[label == -1]\n",
    "\n",
    "    num_ind = len(ind_conf)\n",
    "    num_ood = len(ood_conf)\n",
    "\n",
    "    fp_num = int(np.ceil(fpr * num_ood))\n",
    "    thresh = np.sort(ood_conf)[-fp_num]\n",
    "    num_tp = np.sum((ind_conf > thresh) * (ind_pred == ind_label))\n",
    "    ccr = num_tp / num_ind\n",
    "\n",
    "    return ccr\n",
    "\n",
    "\n",
    "def detection(ind_confidences,\n",
    "              ood_confidences,\n",
    "              n_iter=100000,\n",
    "              return_data=False):\n",
    "    # calculate the minimum detection error\n",
    "    Y1 = ood_confidences\n",
    "    X1 = ind_confidences\n",
    "\n",
    "    start = np.min([np.min(X1), np.min(Y1)])\n",
    "    end = np.max([np.max(X1), np.max(Y1)])\n",
    "    gap = (end - start) / n_iter\n",
    "\n",
    "    best_error = 1.0\n",
    "    best_delta = None\n",
    "    all_thresholds = []\n",
    "    all_errors = []\n",
    "    for delta in np.arange(start, end, gap):\n",
    "        tpr = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
    "        error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
    "        detection_error = (tpr + error2) / 2.0\n",
    "\n",
    "        if return_data:\n",
    "            all_thresholds.append(delta)\n",
    "            all_errors.append(detection_error)\n",
    "\n",
    "        if detection_error < best_error:\n",
    "            best_error = np.minimum(best_error, detection_error)\n",
    "            best_delta = delta\n",
    "\n",
    "    if return_data:\n",
    "        return best_error, best_delta, all_errors, all_thresholds\n",
    "    else:\n",
    "        return best_error, best_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf9faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained_models', padding=True, truncation=True, return_tensors='pt', do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained('./pretrained_models', num_labels=config['K'])\n",
    "# new_classifier = nn.Linear(model.config.hidden_size, config['K'])  # 假设 num_labels 是你的分类数量\n",
    "# model.classifier = new_classifier\n",
    "# print(tokenizer.tokenize('I have a good time, thank you.'))\n",
    "\n",
    "train_data = [{\"tgt_text\": example.tgt_text, \"label\": example.label} for example in dataset['train']]\n",
    "val_data = [{\"tgt_text\": example.tgt_text, \"label\": example.label} for example in dataset['val']]\n",
    "test_data = [{\"tgt_text\": example.tgt_text, \"label\": example.label} for example in dataset['test']]\n",
    "\n",
    "train_dataloader = create_data_loader(train_data, tokenizer, batch_size=config['batch_size'], max_length=max_seq_length)\n",
    "val_dataloader = create_test_data_loader(val_data, tokenizer, batch_size=config['batch_size'], max_length=max_seq_length)\n",
    "test_dataloader = create_test_data_loader(test_data, tokenizer, batch_size=config['batch_size'], max_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3612e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRODNet(nn.Module):\n",
    "    def __init__(self, backbone, feat_dim, num_classes):\n",
    "        super(GRODNet, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        if hasattr(self.backbone, 'fc'):\n",
    "            # remove fc otherwise ddp will\n",
    "            # report unused params\n",
    "            self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.lda = LDA(n_components=feat_dim)\n",
    "        self.pca = PCA(n_components=feat_dim)\n",
    "\n",
    "        self.n_cls = num_classes\n",
    "        self.head1 = nn.Linear(768, 2 * num_classes)\n",
    "        self.head = nn.Linear(768, self.n_cls + 1)\n",
    "        self.k = nn.Parameter(torch.tensor([0.1], dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "    def forward(self, x, y, attention): #x:data feature, y:label\n",
    "        feat = self.backbone.bert(x, attention)[1]#.squeeze() #(b,768)\n",
    "        # output = self.backbone(x)[0].squeeze() #(b,10)\n",
    "        self.lda.fit(feat, y)\n",
    "        X_lda = self.lda.transform(feat) #(b, feat_dim)\n",
    "        \n",
    "        self.pca.fit(feat)\n",
    "        X_pca = self.pca.transform(feat)\n",
    "\n",
    "        return feat, X_lda, X_pca\n",
    "    def intermediate_forward(self, x, attention):\n",
    "        feat = self.backbone.bert(x, attention)[1]\n",
    "        output = self.head(feat)\n",
    "        score = torch.softmax(output, dim=1)\n",
    "        score0 = output[:,:-1]\n",
    "        # score0 = torch.softmax(output[:,:-1], dim=1)\n",
    "        conf = torch.max(score, dim=1)\n",
    "        pred = torch.argmax(score, dim=1)\n",
    "        conf0 = torch.max(score0, dim=1)\n",
    "        pred0 = torch.argmax(score0, dim=1)\n",
    "        for i in range(pred.size(0)):\n",
    "            if pred[i] == output.size(1) - 1:\n",
    "                # conf[i] = 0.1\n",
    "                # pred[i] = 1\n",
    "                score0[i] = 0.1 * torch.ones(score0.size(1)).to(x.device)\n",
    "            # else:\n",
    "                # conf[i] = conf0[i]   \n",
    "        # return score0\n",
    "        return torch.softmax(score0, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class LDA(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        super(LDA, self).__init__()\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            n_samples, n_features = X.shape\n",
    "        except:\n",
    "            n_features = X.shape[0]\n",
    "        classes = torch.unique(y)\n",
    "        n_classes = len(classes)\n",
    "        \n",
    "        means = torch.zeros(n_classes, n_features).to(X.device)\n",
    "        for i, c in enumerate(classes):\n",
    "            try:\n",
    "                means[i] = torch.mean(X[y==c], dim=0)\n",
    "            except:\n",
    "                X = torch.unsqueeze(X, dim=0)\n",
    "                means[i] = torch.mean(X[y==c], dim=0)\n",
    "        \n",
    "        overall_mean = torch.mean(X, dim=0)\n",
    "        \n",
    "        within_class_scatter = torch.zeros(n_features, n_features).to(X.device)\n",
    "        for i, c in enumerate(classes):\n",
    "            class_samples = X[y==c]\n",
    "            deviation = class_samples - means[i]\n",
    "            within_class_scatter += torch.mm(deviation.t(), deviation)\n",
    "        \n",
    "        between_class_scatter = torch.zeros(n_features, n_features).to(X.device)\n",
    "        for i, c in enumerate(classes):\n",
    "            n = len(X[y==c])\n",
    "            mean_diff = (means[i] - overall_mean).unsqueeze(1)\n",
    "            between_class_scatter += n * torch.mm(mean_diff, mean_diff.t())\n",
    "\n",
    "        # torch.backends.cuda.preferred_linalg_library('magma')\n",
    "        # print((torch.inverse(within_class_scatter) @ between_class_scatter).size()) #(768,768)\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(\n",
    "        torch.inverse(within_class_scatter @ between_class_scatter  + 1e-2 * torch.eye((within_class_scatter @ between_class_scatter).size(0)).to(X.device)\n",
    "        ))\n",
    "        _, top_indices = torch.topk(eigenvalues, k=self.n_components, largest=True)\n",
    "        self.components = eigenvectors[:, top_indices]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return torch.mm(X, self.components)\n",
    "\n",
    "class PCA(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        super(PCA, self).__init__()\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        try:\n",
    "            n_samples, n_features = X.shape\n",
    "        except:\n",
    "            n_samples = 1\n",
    "        \n",
    "        self.mean = torch.mean(X, dim=0)\n",
    "        X_centered = X - self.mean\n",
    "        \n",
    "        covariance_matrix = torch.mm(X_centered.t(), X_centered) / max((n_samples - 1),1)\n",
    "        \n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(covariance_matrix)\n",
    "        _, top_indices = torch.topk(eigenvalues, k=self.n_components, largest=True)\n",
    "        self.components = eigenvectors[:, top_indices]\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return torch.mm(X_centered, self.components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d59fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grod v2\n",
    "import faiss.contrib.torch_utils\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from einops import repeat\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "class GRODTrainer_Soft_Label:\n",
    "    def __init__(self, net: nn.Module, train_loader: DataLoader,\n",
    "                 config) -> None:\n",
    "        self.device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = net.to(self.device)  \n",
    "        self.train_loader = train_loader\n",
    "        self.config = config\n",
    "\n",
    "        self.n_cls = config['dataset']['num_classes']\n",
    "\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            params=net.parameters(),\n",
    "            lr=config['optimizer']['lr'],\n",
    "            weight_decay=config['optimizer']['weight_decay'],\n",
    "        )\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max = 10\n",
    "        )\n",
    "\n",
    "        self.head = self.net.head\n",
    "        self.head1 = self.net.head1\n",
    "        self.alpha = config['trainer']['alpha']\n",
    "        self.nums_rounded = config['trainer']['nums_rounded']\n",
    "        self.gamma = config['trainer']['gamma']\n",
    "        self.stat_smooth = 0.3\n",
    "        self.batch_size = config['dataset']['batch_size']\n",
    "        self.threshold = 20\n",
    "        \n",
    "        self.k = self.net.k\n",
    "        \n",
    "        self.best_accuracy = 0.0  # Update best accuracy\n",
    "        self.best_model_state = None  # Save current model state    \n",
    "\n",
    "    def train(self, epochs):\n",
    "        # adjust_learning_rate(self.config, self.optimizer, epoch_idx - 1)\n",
    "        # self.net = nn.DataParallel(self.net)\n",
    "        self.net.train()\n",
    "        self.net.to(self.device)\n",
    "        for epoch_idx in range(epochs):\n",
    "            loss_avg = 0.0\n",
    "            train_dataiter = iter(self.train_loader)\n",
    "    \n",
    "\n",
    "            sub_datasets_in_mu = torch.zeros((self.n_cls, 768)).to(self.device) #(K,f)\n",
    "            dataset_in_mu = torch.zeros(768).to(self.device) #(f)\n",
    "            dataset_in_cov = torch.zeros(768, 768).to(self.device)\n",
    "            sub_datasets_in_cov = torch.zeros((self.n_cls, 768, 768)).to(self.device)\n",
    "            sub_datasets_in_distances = torch.zeros(self.n_cls).to(self.device)\n",
    "            \n",
    "            torch.autograd.detect_anomaly(True)        \n",
    "            \n",
    "            #### Warmup: first x step without lda ood, compute mu and cov for each class instead ###\n",
    "            warmup = int(self.threshold * self.n_cls / self.batch_size)\n",
    "            data_warmup = None\n",
    "            print(\"Warmup...\")\n",
    "            if warmup == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for train_step in tqdm(range(1,\n",
    "                                            warmup + 1),\n",
    "                                    desc='Epoch {:03d}: '.format(epoch_idx),\n",
    "                                    position=0,\n",
    "                                    leave=True):\n",
    "                    with torch.no_grad():\n",
    "                        batch = next(train_dataiter)\n",
    "                        data = batch['input_ids'].to(self.device)\n",
    "                        target = batch['label'].to(self.device)        \n",
    "                        attention_mask = batch['attention_mask'].to(self.device)  \n",
    "                        \n",
    "                        data_in, feat_lda, feat_pca = self.net(data, target, attention_mask)             \n",
    "                        \n",
    "                        if train_step == 1:\n",
    "                            data_warmup = data_in\n",
    "                        else:    \n",
    "                            data_warmup = torch.cat((data_warmup, data_in), dim=0) \n",
    "            \n",
    "            if warmup == 0:\n",
    "                pass\n",
    "            else:        \n",
    "                dataset_in_mu = torch.mean(data_warmup, dim = 0)\n",
    "                cov0 = torch.tensor(self.calculate_covariance_matrix(data_warmup).detach() + 1e-4 * torch.eye(dataset_in_mu.size(0)).to(self.device).detach(), dtype = torch.double)\n",
    "                L = torch.linalg.cholesky(cov0).detach()\n",
    "                L_inv = torch.linalg.inv(L).detach()\n",
    "                dataset_in_cov = torch.tensor(torch.mm(L_inv.t(), L_inv).unsqueeze(0).detach(), dtype=torch.float)\n",
    "                \n",
    "                sub_datasets_in = [Subset(data_warmup, torch.where(target == i)[0]) for i in range(self.n_cls)]      \n",
    "                    \n",
    "\n",
    "                for i in range(len(sub_datasets_in)):\n",
    "                    dataloader = DataLoader(sub_datasets_in[i], batch_size=int(self.threshold * self.n_cls), shuffle=False)\n",
    "                    for batch in dataloader:\n",
    "                        tensor_data_in = batch\n",
    "                    \n",
    "                        mean =  torch.mean(tensor_data_in, dim = 0)\n",
    "                        cov0 = (self.calculate_covariance_matrix(tensor_data_in)+1e-4 * torch.eye(mean.size(0)).to(self.device)).detach()\n",
    "                        L = torch.linalg.cholesky(cov0).detach()\n",
    "                        L_inv = torch.linalg.inv(L).detach()\n",
    "\n",
    "                        # Solve the inverse of a symmetric positive definite matrix A using the inverse of a lower triangular matrix\n",
    "                        cov = torch.mm(L_inv.t(), L_inv)\n",
    "\n",
    "                        sub_datasets_in_cov[i,:,:] = cov.detach()\n",
    "                        sub_datasets_in_mu[i,:] = mean.detach()      \n",
    "                        sub_datasets_in_distances[i] = torch.max(self.mahalanobis(tensor_data_in, sub_datasets_in_mu.clone(), sub_datasets_in_cov.clone())[:,i]).detach()                                              \n",
    "            #### Warmup: first x step without lda ood, compute mu and cov for each class instead ###     \n",
    "            \n",
    "            self.net.train()\n",
    "            for train_step in tqdm(range(warmup + 1,\n",
    "                                        len(train_dataiter)),\n",
    "                                desc='Epoch {:03d}: '.format(epoch_idx),\n",
    "                                position=0,\n",
    "                                leave=True):\n",
    "\n",
    "                batch = next(train_dataiter)\n",
    "                data = batch['input_ids'].to(self.device)\n",
    "                target = batch['label'].to(self.device)        \n",
    "                attention_mask = batch['attention_mask'].to(self.device)  \n",
    "                \n",
    "                data_in, feat_lda, feat_pca = self.net(data, target, attention_mask)    \n",
    "                \n",
    "                \n",
    "                data = data_in\n",
    "                data_in = data_in.detach()\n",
    "                feat_lda = feat_lda.detach()\n",
    "                feat_pca = feat_pca.detach()\n",
    "\n",
    "                # generate rounded ood data\n",
    "                sub_datasets_in = [Subset(data_in, torch.where(target == i)[0]) for i in range(self.n_cls)]\n",
    "                sub_datasets_lda = [Subset(feat_lda, torch.where(target == i)[0]) for i in range(self.n_cls)]   \n",
    "                \n",
    "                # Count the number of samples in each sub-dataset\n",
    "                dataset_lengths = torch.tensor([len(subset) for subset in sub_datasets_lda])\n",
    "                mask = dataset_lengths > 2\n",
    "                lda_class = len(dataset_lengths[mask])\n",
    "                \n",
    "                \n",
    "                reshaped_rounded_data, dataset_in_mu = self.grod_generate_pca(data_in, feat_lda, feat_pca, train_step, dataset_in_mu)\n",
    "                \n",
    "                data = torch.cat((data, reshaped_rounded_data), dim = 0)\n",
    "                \n",
    "                if lda_class > 0:\n",
    "                    reshaped_rounded_data, sub_datasets_in_mu, sub_datasets_in_cov, sub_datasets_in_distances = self.grod_generate_lda(feat_lda, sub_datasets_in, sub_datasets_lda, sub_datasets_in_mu, sub_datasets_in_cov, sub_datasets_in_distances, lda_class)\n",
    "            \n",
    "                    data = torch.cat((data, reshaped_rounded_data), dim = 0)\n",
    "            \n",
    "                data = torch.cat((data, reshaped_rounded_data), dim = 0)\n",
    "                # print(reshaped_rounded_data.size())\n",
    "\n",
    "                    \n",
    "\n",
    "                data_add = data[data_in.size(0):]   \n",
    "                # print(data_add.size())\n",
    "                    \n",
    "                \n",
    "                distances = self.mahalanobis(data_add, sub_datasets_in_mu, sub_datasets_in_cov).to(self.device) #(n,k)\n",
    "                \n",
    "                # Calculate the minimum distance and corresponding category index of each sample point\n",
    "                min_distances, min_distances_clas = torch.min(distances, dim=1)                       \n",
    "                # Get the sub-dataset distance corresponding to each sample point\n",
    "                sub_distances = sub_datasets_in_distances[min_distances_clas.to(self.device)]\n",
    "                \n",
    "                ### soft label of outliers ###\n",
    "                target_add = torch.zeros((data_add.size(0), self.n_cls + 1)).to(self.device) #(n, K+1)\n",
    "                extend = torch.clamp(sub_datasets_in_distances / (distances + 1e-5), -1e5, 1e5)\n",
    "                # print(extend.max())\n",
    "                # print(extend.size(), extend)\n",
    "                target_add[:,:-1] = torch.exp(- (1 - extend))\n",
    "                # extend_ood = torch.gather(extend, 1, min_distances_clas.unsqueeze(1)).squeeze(1).to(self.device)\n",
    "                extend_ood, _ = torch.max(extend, dim=1)\n",
    "                # print(extend_ood)\n",
    "                assert not torch.isnan(extend).any(), \"NaN values found in `extend`\"\n",
    "                assert not torch.isinf(extend).any(), \"Inf values found in `extend`\"\n",
    "                assert not torch.isnan(min_distances_clas).any(), \"NaN values found in `min_distances_clas`\"\n",
    "                target_add[:,-1] = torch.exp(1 - extend_ood)\n",
    "                # print(target_add[:,:-1], target_add[:,-1])\n",
    "                ### soft label of outliers ###\n",
    "                \n",
    "                \n",
    "                \n",
    "                k_init = (torch.mean(min_distances / sub_distances) - 1) * 10\n",
    "                \n",
    "                mask = min_distances > (1 + k_init * self.k.to(self.device)[0]) * sub_distances\n",
    "                # Use Boolean indexing to remove data points that meet a condition\n",
    "                cleaned_data_add = data_add[mask.to(self.device)]\n",
    "                cleaned_target_add = target_add[mask.to(self.device)]\n",
    "                    \n",
    "                if cleaned_data_add.size(0) > data_in.size(0) // self.n_cls + 2:\n",
    "                    indices = torch.randperm(cleaned_data_add.size(0))[:(data_in.size(0) // self.n_cls + 2)].to(self.device)\n",
    "                    cleaned_data_add_de = cleaned_data_add[indices]\n",
    "                    cleaned_target_add_de = cleaned_target_add[indices]\n",
    "                else: \n",
    "                    cleaned_data_add_de = cleaned_data_add\n",
    "                    cleaned_target_add_de = cleaned_target_add\n",
    "                    \n",
    "                data = torch.cat((data[:data_in.size(0)], cleaned_data_add_de), dim = 0)\n",
    "                \n",
    "                target = F.one_hot(target, num_classes=self.n_cls + 1)\n",
    "                \n",
    "                target = torch.cat((target, cleaned_target_add_de), dim = 0)\n",
    "                # print(target.size())\n",
    "                \n",
    "                    \n",
    "\n",
    "                output = self.head(data)\n",
    "                # print(output.size())\n",
    "                # output = F.normalize(output, dim=1)\n",
    "                loss1 = F.cross_entropy(output, target)\n",
    "\n",
    "                label_matrix = output\n",
    "                biclas = torch.zeros(label_matrix.size(0), 2)\n",
    "                biclas[:,-1] = label_matrix[:,-1]\n",
    "                biclas[:,0] = torch.sum(label_matrix[:,:-1],-1)\n",
    "                \n",
    "                label_biclas = torch.zeros(target.size(0), 2)\n",
    "                label_biclas[:,-1] = target[:,-1]\n",
    "                label_biclas[:,0] = torch.sum(target[:,:-1],-1)\n",
    "                \n",
    "                loss2 = F.cross_entropy(biclas.to(self.device), label_biclas.to(self.device))\n",
    "                # print(loss1, loss2, cleaned_target_add_de) \n",
    "                loss = (1 - self.gamma) * loss1 + self.gamma * loss2 \n",
    "                \n",
    "                # backward\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "                # exponential moving average, show smooth values\n",
    "                with torch.no_grad():\n",
    "                    loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "\n",
    "            print(f'Epoch {epoch_idx + 1}/{len(train_dataiter)}, Average Training Loss: {loss_avg:.4f}')\n",
    "            # accuracy = correct / total\n",
    "            # print(f'Accuracy on validation set: {accuracy:.4f}')\n",
    "            accuracy = self.test_model()  # Test model after each epoch\n",
    "            if accuracy > self.best_accuracy or accuracy == self.best_accuracy:  # If current accuracy is better than best\n",
    "                self.best_accuracy = accuracy  # Update best accuracy\n",
    "                self.best_model_state = self.net.state_dict()  # Save current model state                \n",
    "                \n",
    "    def test_model(self):\n",
    "        self.net.eval()  # Switch to evaluation mode\n",
    "        self.net.to(self.device)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_dataiter = iter(self.train_loader)\n",
    "        with torch.no_grad():\n",
    "            for train_step in tqdm(range(1, len(val_dataiter) + 1),\n",
    "                                    position=0,\n",
    "                                    leave=True):\n",
    "                batch = next(val_dataiter)\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                data_in, feat_lda, feat_pca = self.net(input_ids, labels, attention_mask)\n",
    "                outputs = self.head(data_in)\n",
    "                # outputs = self.net.backbone(input_ids, attention_mask).logits\n",
    "                predicted = torch.argmax(outputs[:,:-1], dim=1)\n",
    "                # print(predicted, labels)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        print(f'Accuracy on validation set: {accuracy:.4f}')\n",
    "        return accuracy    \n",
    "\n",
    "    def save_best_model(self, filename):\n",
    "        if self.best_model_state is not None:\n",
    "            torch.save(self.best_model_state, filename)\n",
    "\n",
    "    \n",
    "    def grod_generate_pca(self, data_in, feat_lda, feat_pca, train_step, dataset_in_mu):\n",
    "        # generate PCA ood data\n",
    "        argmax = torch.zeros(feat_pca.size()[1])\n",
    "        argmax = torch.argmax(feat_pca,dim=0) #feat_dim\n",
    "        \n",
    "        argmin = torch.zeros(feat_pca.size()[1])\n",
    "        argmin = torch.argmin(feat_pca,dim=0) #feat_dim\n",
    "\n",
    "        for j in range(feat_pca.size()[1]):\n",
    "            if j==0:\n",
    "                pcadata_rounded_category = data_in[int(argmax[j].item())].unsqueeze(0)\n",
    "                pcadata_rounded_category_1 = data_in[int(argmin[j].item())].unsqueeze(0)\n",
    "            else:\n",
    "                \n",
    "                pcadata_rounded_category = torch.cat((pcadata_rounded_category, data_in[int(argmax[j].item())].unsqueeze(0)),dim=0)\n",
    "                pcadata_rounded_category_1 = torch.cat((pcadata_rounded_category_1, data_in[int(argmin[j].item())].unsqueeze(0)),dim=0)\n",
    "        \n",
    "        ### mu and std smoothing ###\n",
    "        if train_step == 1:\n",
    "            dataset_in_mu = torch.mean(data_in, dim = 0)\n",
    "        else:\n",
    "            dataset_in_mu = (1 - self.stat_smooth) * torch.mean(data_in.detach().clone(), dim = 0) + self.stat_smooth * dataset_in_mu.detach().clone() \n",
    "        ### mu and std smoothing ###\n",
    "        \n",
    "        # dataset_in_mu = torch.mean(data_in.detach().clone(), dim = 0)\n",
    "        \n",
    "        dataset_in_mu =  repeat(dataset_in_mu.squeeze(), \"f -> b f\", \n",
    "                                        f = data_in.size(1), b = feat_lda.size()[1])\n",
    "        # print(data_rounded_category.size())\n",
    "        B = pcadata_rounded_category.detach()\n",
    "        B_1 = pcadata_rounded_category_1.detach()\n",
    "        # print(A.size())\n",
    "        pcavector = F.normalize(B.clone() - dataset_in_mu, dim = 1)\n",
    "        pcavector_1 = F.normalize(B_1.clone() - dataset_in_mu, dim = 1)\n",
    "        B = torch.add(B, self.alpha * pcavector).detach() #(feat_dim, 768)\n",
    "        B_1 = torch.add(B_1, self.alpha * pcavector_1).detach() #(feat_dim, 768)\n",
    "        mean_matrix_0 = B\n",
    "        mean_matrix_1 = B_1\n",
    "        # print(A.size())\n",
    "        mean_matrix = torch.cat((mean_matrix_0, mean_matrix_1), dim = 0)\n",
    "        # mean_matrix = mean_matrix_0\n",
    "        std = 1 / 3 * self.alpha\n",
    "        mu = mean_matrix.T.unsqueeze(2).to(self.device) \n",
    "        rand_data = torch.randn(mean_matrix.size(1), self.nums_rounded).to(self.device) \n",
    "        gaussian_data = mu + std * rand_data.unsqueeze(1) #(768, num, nums_rounded)\n",
    "        # print(gaussian_data.size())\n",
    "        nums = gaussian_data.size(1)\n",
    "        nums_rounded = gaussian_data.size(2)\n",
    "        reshaped_rounded_data = gaussian_data.permute(1, 2, 0).contiguous().view(nums * nums_rounded, mean_matrix.size(1)) # (num* nums_rounded, 768)\n",
    "        # print(reshaped_rounded_data.size(),data.size())\n",
    "        return reshaped_rounded_data, dataset_in_mu\n",
    "        \n",
    "    def grod_generate_lda(self, feat_lda, sub_datasets_in, sub_datasets_lda, sub_datasets_in_mu, sub_datasets_in_cov, sub_datasets_in_distances, lda_class):   \n",
    "        dataset_lengths = torch.tensor([len(subset) for subset in sub_datasets_lda])\n",
    "        # Get the index of sub-datasets with the largest amount of data\n",
    "        top_indices = sorted(range(len(dataset_lengths)), key=lambda i: dataset_lengths[i], reverse=True)[:lda_class]\n",
    "\n",
    "        \n",
    "        arg_max = torch.zeros((len(sub_datasets_lda), feat_lda.size()[1]))\n",
    "        arg_min = torch.zeros((len(sub_datasets_lda), feat_lda.size()[1]))\n",
    "        k = 0\n",
    "        for i in top_indices:\n",
    "            k = k + 1\n",
    "            dataloader = DataLoader(sub_datasets_lda[i], batch_size=64, shuffle=False)\n",
    "            for batch in dataloader:\n",
    "                tensor_data_lda = batch\n",
    "            dataloader = DataLoader(sub_datasets_in[i], batch_size=64, shuffle=False)\n",
    "            for batch in dataloader:\n",
    "                tensor_data_in = batch\n",
    "            arg_max[i] = torch.argmax(tensor_data_lda, dim=0) #feat_dim                   \n",
    "            arg_min[i] = torch.argmin(tensor_data_lda, dim=0) #feat_dim\n",
    "\n",
    "            for j in range(feat_lda.size()[1]):\n",
    "                # print(argmax[i][j].item())\n",
    "                if k == 1 and j==0:\n",
    "                    data_rounded_category = tensor_data_in[int(arg_max[i][j].item())].unsqueeze(0)\n",
    "                    data_rounded_category_1 = tensor_data_in[int(arg_min[i][j].item())].unsqueeze(0)\n",
    "                else:\n",
    "                    data_rounded_category = torch.cat((data_rounded_category, tensor_data_in[int(arg_max[i][j].item())].unsqueeze(0)),dim=0)\n",
    "                    data_rounded_category_1 = torch.cat((data_rounded_category_1, tensor_data_in[int(arg_min[i][j].item())].unsqueeze(0)),dim=0)\n",
    "\n",
    "            if tensor_data_in.size(0) > 1:\n",
    "                \n",
    "                mean =  torch.mean(tensor_data_in, dim = 0)\n",
    "                cov0 = (self.calculate_covariance_matrix(tensor_data_in)+1e-4 * torch.eye(mean.size(0)).to(self.device)).detach()\n",
    "                L = torch.linalg.cholesky(cov0).detach()\n",
    "                L_inv = torch.linalg.inv(L).detach()\n",
    "\n",
    "                # Solve the inverse of a symmetric positive definite matrix A using the inverse of a lower triangular matrix\n",
    "                cov = torch.mm(L_inv.t(), L_inv)\n",
    "                ### mu and std smoothing ###\n",
    "                if torch.max(torch.abs(sub_datasets_in_mu[i,:]))<1e-7:\n",
    "                    sub_datasets_in_cov[i,:,:] = cov.detach()\n",
    "                    sub_datasets_in_mu[i,:] = mean.detach()                        \n",
    "                    sub_datasets_in_distances[i] = torch.max(self.mahalanobis(tensor_data_in, sub_datasets_in_mu.clone(), sub_datasets_in_cov.clone())[:,i]).detach()                                                                     \n",
    "                else:\n",
    "                    sub_datasets_in_cov[i,:,:] = (1 - self.stat_smooth) * cov.detach().clone().to(self.device) + self.stat_smooth * sub_datasets_in_cov[i,:,:].detach().clone()\n",
    "                    sub_datasets_in_mu[i,:] = (1 - self.stat_smooth) * mean.detach().clone().to(self.device) + self.stat_smooth * sub_datasets_in_mu[i,:].detach().clone()\n",
    "                    dists = self.mahalanobis(tensor_data_in, sub_datasets_in_mu.clone(), sub_datasets_in_cov.clone())[:,i]\n",
    "                    dist = torch.max(dists)\n",
    "                    sub_datasets_in_distances[i] = (1 - self.stat_smooth) * dist.to(self.device).detach().clone() + self.stat_smooth * sub_datasets_in_distances[i].detach().clone()\n",
    "                ### mu and std smoothing ###\n",
    "            \n",
    "            \n",
    "            sub_datasets_in_mean =  repeat(sub_datasets_in_mu.clone()[i,:], \"f -> b f\", \n",
    "                                        f = tensor_data_in.size(1), b = feat_lda.size()[1])\n",
    "            \n",
    "            A = data_rounded_category[-feat_lda.size()[1]:].detach()\n",
    "            A_1 = data_rounded_category_1[- feat_lda.size()[1]:].detach()\n",
    "            vector = F.normalize(A.to(self.device) - sub_datasets_in_mean.to(self.device), dim = 1)\n",
    "            vector_1 = F.normalize(A_1.to(self.device) - sub_datasets_in_mean.to(self.device), dim = 1)\n",
    "            A = A + self.alpha * vector.detach().to(self.device) #(feat_dim, 768)\n",
    "            A_1 = A_1 + self.alpha * vector_1.detach().to(self.device) #(feat_dim, 768)\n",
    "            if k == 1:\n",
    "                mean_matrix_0 = A\n",
    "                mean_matrix_1 = A_1\n",
    "            else:\n",
    "                mean_matrix_0 = torch.cat((mean_matrix_0, A), dim = 0) #(num, 768)\n",
    "                mean_matrix_1 = torch.cat((mean_matrix_1, A_1), dim = 0) #(num, 768)\n",
    "            mean_matrix = torch.cat((mean_matrix_0, mean_matrix_1), dim = 0)\n",
    "            # print(mean_matrix.size())\n",
    "            std = 1 / 3 * self.alpha\n",
    "            mu = mean_matrix.T.unsqueeze(2).to(self.device) #(768,num,1)\n",
    "            rand_data = torch.randn(mean_matrix.size(1), self.nums_rounded).to(self.device) #(768,nums_rounded)\n",
    "            gaussian_data = mu + std * rand_data.unsqueeze(1) #(768, num, nums_rounded)\n",
    "            # print(gaussian_data.size())\n",
    "            nums = gaussian_data.size(1)\n",
    "            nums_rounded = gaussian_data.size(2)\n",
    "            reshaped_rounded_data = gaussian_data.permute(1, 2, 0).contiguous().view(nums * nums_rounded, mean_matrix.size(1)) # (num* nums_rounded, 768)\n",
    "            \n",
    "            return reshaped_rounded_data, sub_datasets_in_mu, sub_datasets_in_cov, sub_datasets_in_distances\n",
    "    \n",
    "    def mahalanobis(self, x, support_mean, inv_covmat): #(n,d), (k,d), (k,d,d)\n",
    "        n = x.size(0)\n",
    "        d = x.size(1)\n",
    "\n",
    "        x = x.to(inv_covmat.device)\n",
    "        support_mean = support_mean.to(inv_covmat.device)\n",
    "\n",
    "        maha_dists = []\n",
    "        for i in range(inv_covmat.size(0)):\n",
    "            class_inv_cov = inv_covmat[i].detach()\n",
    "            support_class = support_mean[i].detach()\n",
    "        \n",
    "            x_mu = x - support_class.unsqueeze(0).expand(n, d)            \n",
    "            class_inv_cov = class_inv_cov.to(inv_covmat.device)\n",
    "\n",
    "            # Mahalanobis distances\n",
    "            left = torch.matmul(x_mu, class_inv_cov)\n",
    "            # print(x_mu.size(), class_inv_cov.size(), left.size())\n",
    "            mahal = torch.matmul(left, x_mu.t()).diagonal()\n",
    "            maha_dists.append(mahal)\n",
    "\n",
    "        return torch.stack(maha_dists).t()\n",
    "    \n",
    "    def calculate_covariance_matrix(self, data):\n",
    "        mean = torch.mean(data, dim=0)\n",
    "        mean = mean.unsqueeze(0).expand(data.size(0), data.size(1))\n",
    "        centered_data = data - mean\n",
    "\n",
    "        covariance_matrix = torch.mm(centered_data.t(), centered_data) / (centered_data.size(0) - 1 + 1e-7)\n",
    "\n",
    "        return covariance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = 'grod copy.yml'\n",
    "loaded_parameters = load_parameters_from_yaml(yaml_file_path)\n",
    "config_grod = loaded_parameters\n",
    "        \n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model_grod = GRODNet(model, 1, config_grod['dataset']['num_classes']).to(device)\n",
    "trainer = GRODTrainer_Soft_Label(model_grod, train_dataloader, config_grod)\n",
    "trainer.train(config_grod['optimizer']['num_epochs']) \n",
    "\n",
    "# Save the best model state\n",
    "trainer.save_best_model('best_model_grod2_clinc_0.001.ckpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56256d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载保存的状态字典\n",
    "state_dict = torch.load('best_model_grod2_clinc_0.001.ckpt')\n",
    "model_grod = GRODNet(model, 1, config_grod['dataset']['num_classes'])\n",
    "# 将加载的状态字典加载到模型中\n",
    "model_grod.load_state_dict(state_dict)\n",
    "print(model_grod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f342109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from numpy.linalg import norm, pinv\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from tqdm import tqdm\n",
    "\n",
    "# run\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class BasePostprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def setup(self, net: nn.Module, id_loader_dict, ood_loader_dict):\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def postprocess(self, net: nn.Module, data: Any, attention):\n",
    "        output = net(input_ids=data, attention_mask=attention)\n",
    "        score = torch.softmax(output.logits, dim=1)\n",
    "        conf, pred = torch.max(score, dim=1)\n",
    "        return pred, conf\n",
    "\n",
    "    def inference(self,\n",
    "                  net: nn.Module,\n",
    "                  data_loader: DataLoader, \n",
    "                  alpha, w, b, u, NS,\n",
    "                  progress: bool = True):\n",
    "        pred_list, conf_list, label_list = [], [], []\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].cuda()\n",
    "            attention_mask = batch[\"attention_mask\"].cuda()\n",
    "            labels = batch['label'].cuda()\n",
    "            pred, conf = self.postprocess(net.cuda(), input_ids, attention_mask, alpha, w, b, u, NS,)\n",
    "\n",
    "            pred_list.append(pred.cpu())\n",
    "            conf_list.append(conf.cpu())\n",
    "            label_list.append(labels.cpu())\n",
    "\n",
    "        # convert values into numpy array\n",
    "        pred_list = torch.cat(pred_list).numpy().astype(int)\n",
    "        conf_list = torch.cat(conf_list).numpy()\n",
    "        label_list = torch.cat(label_list).numpy().astype(int)\n",
    "\n",
    "        return pred_list, conf_list, label_list\n",
    "\n",
    "\n",
    "class GRODPostprocessor(BasePostprocessor):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.args = self.config['postprocessor']['postprocessor_args']\n",
    "        self.args_dict = self.config['postprocessor']['postprocessor_sweep']\n",
    "        self.dim = self.args['dim']\n",
    "        self.setup_flag = False\n",
    "\n",
    "    def setup(self, net: nn.Module, id_loader_dict):\n",
    "        if not self.setup_flag:\n",
    "            net.eval()\n",
    "            net.cuda()\n",
    "            with torch.no_grad():\n",
    "                # self.w, self.b = net.backbone.get_fc()\n",
    "                # print(self.b.size)\n",
    "                self.w, self.b = net.head.weight[:-1,:].cpu().numpy(), net.head.bias[:-1].cpu().numpy()\n",
    "                # print(self.w.size())\n",
    "                print('Extracting id training feature')\n",
    "                feature_id_train = []\n",
    "                logit_id_train = []\n",
    "                for batch in tqdm(id_loader_dict,\n",
    "                                  desc='Setup: ',\n",
    "                                  position=0,\n",
    "                                  leave=True):\n",
    "                    data = batch['input_ids'].cuda()\n",
    "                    attention_mask = batch[\"attention_mask\"].cuda()\n",
    "                    labels = batch['label'].cuda()\n",
    "                    # data = data.float()\n",
    "                    feature = net.backbone.bert(data, attention_mask)[1]\n",
    "                    logit = net.head(feature)\n",
    "                    score = torch.softmax(logit, dim=1)\n",
    "                    score0 = torch.softmax(logit[:,:-1], dim=1)\n",
    "                    conf, pred = torch.max(score, dim=1)\n",
    "                    conf0, pred0 = torch.max(score0, dim=1)\n",
    "                    for i in range(pred.size(0)):\n",
    "                        if pred[i] == logit.size(1) - 1:\n",
    "                            conf[i] = 0.1\n",
    "                            pred[i] = 1\n",
    "                            score0[i, :] = 0.1 * torch.ones(score0.size(1)).cuda()\n",
    "                        else:\n",
    "                            conf[i] = conf0[i]     \n",
    "                        \n",
    "                    feature_id_train.append(feature.cpu().numpy())\n",
    "                    logit_id_train.append(score0.cpu().numpy())\n",
    "                feature_id_train = np.concatenate(feature_id_train, axis=0)\n",
    "                logit_id_train = np.concatenate(logit_id_train, axis=0)\n",
    "\n",
    "                # logit_id_train = feature_id_train @ self.w.T + self.b\n",
    "\n",
    "            self.u = -np.matmul(pinv(self.w), self.b)\n",
    "            ec = EmpiricalCovariance(assume_centered=True)\n",
    "            ec.fit(feature_id_train - self.u)\n",
    "            eig_vals, eigen_vectors = np.linalg.eig(ec.covariance_)\n",
    "            self.NS = np.ascontiguousarray(\n",
    "                (eigen_vectors.T[np.argsort(eig_vals * -1)[self.dim:]]).T)\n",
    "\n",
    "            vlogit_id_train = norm(np.matmul(feature_id_train - self.u,\n",
    "                                             self.NS),\n",
    "                                   axis=-1)\n",
    "            \n",
    "            print(feature_id_train - self.u, self.NS)\n",
    "            \n",
    "            self.alpha = logit_id_train.max(\n",
    "                axis=-1).mean() / vlogit_id_train.mean()\n",
    "            print(f'{self.alpha=:.4f}')\n",
    "\n",
    "            self.setup_flag = True\n",
    "        else:\n",
    "            pass\n",
    "        return self.alpha, self.w, self.b, self.u, self.NS\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def postprocess(self, net: nn.Module, data: Any, attention, alpha, w, b, u, NS):\n",
    "        feature_ood = net.backbone.bert(data, attention)[1]\n",
    "        \n",
    "        logit = net.head(feature_ood)\n",
    "        score = torch.softmax(logit, dim=1)\n",
    "        score0 = torch.softmax(logit[:,:-1], dim=1)\n",
    "        conf, pred = torch.max(score, dim=1)\n",
    "        conf0, pred0 = torch.max(score0, dim=1)\n",
    "        for i in range(pred.size(0)):\n",
    "          if pred[i] == logit.size(1) - 1:\n",
    "            conf[i] = 0.1\n",
    "            pred[i] = 1\n",
    "            score0[i, :] = 0.1 * torch.ones(score0.size(1)).cuda()\n",
    "          else:\n",
    "            conf[i] = conf0[i]\n",
    "        logit_ood = score0.cpu()    \n",
    "        \n",
    "        feature_ood = feature_ood.cpu()\n",
    "        \n",
    "        # logit_ood = feature_ood @ w.T + b\n",
    "        _, pred = torch.max(logit_ood, dim=1)\n",
    "        energy_ood = logsumexp(logit_ood.numpy(), axis=-1)\n",
    "        vlogit_ood = norm(np.matmul(feature_ood.numpy() - u, NS),\n",
    "                          axis=-1) * alpha\n",
    "        score_ood = -vlogit_ood + energy_ood\n",
    "        return pred, torch.from_numpy(score_ood)\n",
    "\n",
    "    def set_hyperparam(self, hyperparam: list):\n",
    "        self.dim = hyperparam[0]\n",
    "\n",
    "    def get_hyperparam(self):\n",
    "        return self.dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = 'grod copy.yml'\n",
    "loaded_parameters = load_parameters_from_yaml(yaml_file_path)\n",
    "config_grod = loaded_parameters\n",
    "alpha, w, b, u, NS = GRODPostprocessor(config_grod).setup(model_grod, train_dataloader)\n",
    "\n",
    "pred_list, conf_list, label_list = GRODPostprocessor(config_grod).inference(model_grod, test_dataloader, alpha, w, b, u, NS)\n",
    "# conf_list = 1e7 * np.ones(conf_list.shape[0])\n",
    "# [fpr, auroc, aupr_in, aupr_out, accuracy]\n",
    "compute_all_metrics(conf_list, label_list, pred_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OOD-Trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
