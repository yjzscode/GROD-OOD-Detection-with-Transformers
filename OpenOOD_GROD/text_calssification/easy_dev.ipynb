{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "# 数据集处理\n",
    "# imdb\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import os\n",
    "import json, csv\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "from openprompt.utils.logging import logger\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "\n",
    "class IMDBProcessor(DataProcessor):\n",
    "\n",
    "    def __init__(self, is_id_only=True, mode=None):\n",
    "        super().__init__()\n",
    "        self.labels = None\n",
    "        self.is_id_only = is_id_only\n",
    "        self.monitor_mode = 'label'\n",
    "\n",
    "    def get_examples(self, data_dir: str, split: str, frac: int = 0) -> List[InputExample]:\n",
    "        examples = []\n",
    "        if split == 'train' and frac:\n",
    "            path = os.path.join(data_dir, \"{}_{}.csv\".format(split, frac))\n",
    "        else:\n",
    "            path = os.path.join(data_dir, \"{}.csv\".format(split))\n",
    "        data = pd.read_csv(path)\n",
    "        intents = list(data[\"intent\"])\n",
    "        # domian_index = list(data['domain_index'])\n",
    "        indexs = list(data[\"index\"])\n",
    "        utts = list(data[\"utt\"])\n",
    "        id_utt = [utts[i] for i in range(len(utts)) if indexs[i] != -1]\n",
    "        # id_intent = [intents[i] for i in range(len(utts)) if indexs[i] != -1]\n",
    "        ood_utt = [utts[i] for i in range(len(utts)) if indexs[i] == -1]\n",
    "        if self.is_id_only:\n",
    "            utts = id_utt\n",
    "            is_oods = [0 for _ in range(len(utts))]\n",
    "            new_indexs = [indexs[i] for i in range(len(indexs)) if indexs[i] != -1]\n",
    "            # new_domian_index = [domian_index[i] for i in range(len(domian_index)) if domian_index[i] != -1]\n",
    "        else:\n",
    "            utts = id_utt + ood_utt\n",
    "            is_oods = [0 for _ in range(len(id_utt))] + [1 for _ in range(len(ood_utt))]\n",
    "            new_indexs = [indexs[i] for i in range(len(indexs)) if indexs[i] != -1] + [-1 for _ in range(len(ood_utt))]\n",
    "            # new_domian_index = [domian_index[i] for i in range(len(domian_index)) if domian_index[i] != -1] + [-1 for _ in range(len(ood_utt))]\n",
    "            assert len(new_indexs) == len(utts)\n",
    "            # assert len(new_domian_index) == len(utts)\n",
    "        if self.monitor_mode == 'label':\n",
    "            monitor = new_indexs\n",
    "        # elif self.monitor_mode == 'domain':\n",
    "        #     monitor = new_domian_index\n",
    "        for i, (tgt, is_ood, intent) in enumerate(zip(utts, is_oods, new_indexs)):\n",
    "            example = InputExample(guid=str(i), text_a=\"\", tgt_text=tgt, meta={\"is_ood\": is_ood},\n",
    "                                   label=intent if is_ood == 0 else -1)  # label=intent  tgt_text=\"the intent is\"\n",
    "            examples.append(example)\n",
    "        return examples\n",
    "\n",
    "    def get_src_tgt_len_ratio(self, ):\n",
    "        pass\n",
    "\n",
    "    def get_label_words(self, data_dir):\n",
    "        path = os.path.join(data_dir, \"train.csv\")\n",
    "        data = pd.read_csv(path)\n",
    "        intents = list(data[\"intent\"])\n",
    "        domian_index = list(data['index'])\n",
    "        result = collections.defaultdict(str)\n",
    "        for intent, index in zip(intents, domian_index):\n",
    "            result[index] = intent\n",
    "        result = sorted(result.items(), key=lambda x: x[0])\n",
    "        result = [[b] for a, b in result]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c195472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "# clinc\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import os\n",
    "import json, csv\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "from openprompt.utils.logging import logger\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "\n",
    "\n",
    "class ClincProcessor(DataProcessor):\n",
    "\n",
    "    def __init__(self, is_id_only=True, mode=None):\n",
    "        super().__init__()\n",
    "        self.labels = None\n",
    "        self.is_id_only = is_id_only\n",
    "        self.monitor_mode = 'label' if mode == 150 else 'domain'\n",
    "\n",
    "    def get_examples(self, data_dir: str, split: str, frac: int = 0) -> List[InputExample]:\n",
    "        examples = []\n",
    "        if split == 'train' and frac:\n",
    "            path = os.path.join(data_dir, \"{}_{}.csv\".format(split, frac))\n",
    "        else:\n",
    "            path = os.path.join(data_dir, \"{}.csv\".format(split))\n",
    "        data = pd.read_csv(path)\n",
    "        intents = list(data[\"intent\"])\n",
    "        domian_index = list(data['domain_index'])\n",
    "        indexs = list(data[\"index\"])\n",
    "        utts = list(data[\"utt\"])\n",
    "        id_utt = [utts[i] for i in range(len(utts)) if indexs[i] != -1]\n",
    "        # id_intent = [intents[i] for i in range(len(utts)) if indexs[i] != -1]\n",
    "        ood_utt = [utts[i] for i in range(len(utts)) if indexs[i] == -1]\n",
    "        if self.is_id_only:\n",
    "            utts = id_utt\n",
    "            is_oods = [0 for _ in range(len(utts))]\n",
    "            new_indexs = [indexs[i] for i in range(len(indexs)) if indexs[i] != -1]\n",
    "            new_domian_index = [domian_index[i] for i in range(len(domian_index)) if domian_index[i] != -1]\n",
    "        else:\n",
    "            utts = id_utt + ood_utt\n",
    "            is_oods = [0 for _ in range(len(id_utt))] + [1 for _ in range(len(ood_utt))]\n",
    "            new_indexs = [indexs[i] for i in range(len(indexs)) if indexs[i] != -1] + [-1 for _ in range(len(ood_utt))]\n",
    "            new_domian_index = [domian_index[i] for i in range(len(domian_index)) if domian_index[i] != -1] + [-1 for _\n",
    "                                                                                                               in range(\n",
    "                    len(ood_utt))]\n",
    "            assert len(new_indexs) == len(utts)\n",
    "            assert len(new_domian_index) == len(utts)\n",
    "        if self.monitor_mode == 'label':\n",
    "            monitor = new_indexs\n",
    "        elif self.monitor_mode == 'domain':\n",
    "            monitor = new_domian_index\n",
    "        for i, (tgt, is_ood, intent) in enumerate(zip(utts, is_oods, monitor)):\n",
    "            example = InputExample(guid=str(i), text_a=\"\", tgt_text=tgt, meta={\"is_ood\": is_ood},\n",
    "                                   label=intent)  # label=intent  tgt_text=\"the intent is\"\n",
    "            examples.append(example)\n",
    "\n",
    "        return examples\n",
    "\n",
    "    def get_src_tgt_len_ratio(self, ):\n",
    "        pass\n",
    "\n",
    "    def get_label_words(self, data_dir):\n",
    "        path = os.path.join(data_dir, \"train.csv\")\n",
    "        data = pd.read_csv(path)\n",
    "        intents = list(data[\"intent\"])\n",
    "        domian_index = list(data['index'])\n",
    "        result = collections.defaultdict(str)\n",
    "        for intent, index in zip(intents, domian_index):\n",
    "            result[index] = intent\n",
    "        result = sorted(result.items(), key=lambda x: x[0])\n",
    "        result = [[b] for a, b in result]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d10457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import yaml\n",
    "def load_parameters_from_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        parameters = yaml.safe_load(file)\n",
    "    return parameters\n",
    "yaml_file_path = 'paras.yml'\n",
    "loaded_parameters = load_parameters_from_yaml(yaml_file_path)\n",
    "config = loaded_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "if config['dataset'] == 'IMDB':\n",
    "    OOD_DataProcessor = IMDBProcessor\n",
    "    datasets_dir = \"./datasets/imdb_yelp\"\n",
    "    max_seq_length = 256\n",
    "    batch_size = config['batch_size']\n",
    "\n",
    "elif config['dataset'] == 'clinc':\n",
    "    OOD_DataProcessor = ClincProcessor \n",
    "    datasets_dir = \"./datasets/clinc150/\"\n",
    "    max_seq_length = 128\n",
    "    batch_size = config['batch_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "dataset = {}\n",
    "dataset['train'] = OOD_DataProcessor(True).get_examples(datasets_dir, \"train\")\n",
    "dataset['val'] = OOD_DataProcessor(True).get_examples(datasets_dir, \"valid\")\n",
    "\n",
    "dataset[\"val_ood\"] = OOD_DataProcessor(False).get_examples(datasets_dir, \"valid\")\n",
    "dataset['test'] = OOD_DataProcessor(False).get_examples(datasets_dir, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c949c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "# dataloader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from typing import List, Dict\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, str]], tokenizer: BertTokenizer, max_length: int = 128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"tgt_text\"]\n",
    "        label = self.data[idx][\"label\"]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_data_loader(data: List[Dict[str, str]], tokenizer: BertTokenizer, batch_size: int = 32, max_length: int = 128):\n",
    "    dataset = TextClassificationDataset(data, tokenizer, max_length)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "def create_test_data_loader(data: List[Dict[str, str]], tokenizer: BertTokenizer, batch_size: int = 32, max_length: int = 128):\n",
    "    dataset = TextClassificationDataset(data, tokenizer, max_length)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da095890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "# 评价指标\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def compute_all_metrics(conf, label, pred):\n",
    "    np.set_printoptions(precision=3)\n",
    "    recall = 0.95\n",
    "    auroc, aupr_in, aupr_out, fpr = auc_and_fpr_recall(conf, label, recall)\n",
    "\n",
    "    accuracy = acc(pred, label)\n",
    "\n",
    "    results = [fpr, auroc, aupr_in, aupr_out, accuracy]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# accuracy\n",
    "def acc(pred, label):\n",
    "    ind_pred = pred[label != -1] #id acc\n",
    "    ind_label = label[label != -1]\n",
    "\n",
    "    # ind_pred = pred #all acc\n",
    "    # ind_label = label\n",
    "\n",
    "    num_tp = np.sum(ind_pred == ind_label)\n",
    "    acc = num_tp / len(ind_label)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "# fpr_recall\n",
    "def fpr_recall(conf, label, tpr):\n",
    "    gt = np.ones_like(label)\n",
    "    gt[label == -1] = 0\n",
    "\n",
    "    fpr_list, tpr_list, threshold_list = metrics.roc_curve(gt, conf)\n",
    "    fpr = fpr_list[np.argmax(tpr_list >= tpr)]\n",
    "    thresh = threshold_list[np.argmax(tpr_list >= tpr)]\n",
    "    return fpr, thresh\n",
    "\n",
    "\n",
    "# auc\n",
    "def auc_and_fpr_recall(conf, label, tpr_th):\n",
    "    # following convention in ML we treat OOD as positive\n",
    "    ood_indicator = np.zeros_like(label)\n",
    "    ood_indicator[label == -1] = 1\n",
    "\n",
    "    # in the postprocessor we assume ID samples will have larger\n",
    "    # \"conf\" values than OOD samples\n",
    "    # therefore here we need to negate the \"conf\" values\n",
    "    \n",
    "    fpr_list, tpr_list, thresholds = metrics.roc_curve(ood_indicator, -conf)\n",
    "    fpr = fpr_list[np.argmax(tpr_list >= tpr_th)]\n",
    "\n",
    "    precision_in, recall_in, thresholds_in \\\n",
    "        = metrics.precision_recall_curve(1 - ood_indicator, conf)\n",
    "\n",
    "    precision_out, recall_out, thresholds_out \\\n",
    "        = metrics.precision_recall_curve(ood_indicator, -conf)\n",
    "\n",
    "    auroc = metrics.auc(fpr_list, tpr_list)\n",
    "    aupr_in = metrics.auc(recall_in, precision_in)\n",
    "    aupr_out = metrics.auc(recall_out, precision_out)\n",
    "\n",
    "    return auroc, aupr_in, aupr_out, fpr\n",
    "\n",
    "\n",
    "# ccr_fpr\n",
    "def ccr_fpr(conf, fpr, pred, label):\n",
    "    ind_conf = conf[label != -1]\n",
    "    ind_pred = pred[label != -1]\n",
    "    ind_label = label[label != -1]\n",
    "\n",
    "    ood_conf = conf[label == -1]\n",
    "\n",
    "    num_ind = len(ind_conf)\n",
    "    num_ood = len(ood_conf)\n",
    "\n",
    "    fp_num = int(np.ceil(fpr * num_ood))\n",
    "    thresh = np.sort(ood_conf)[-fp_num]\n",
    "    num_tp = np.sum((ind_conf > thresh) * (ind_pred == ind_label))\n",
    "    ccr = num_tp / num_ind\n",
    "\n",
    "    return ccr\n",
    "\n",
    "\n",
    "def detection(ind_confidences,\n",
    "              ood_confidences,\n",
    "              n_iter=100000,\n",
    "              return_data=False):\n",
    "    # calculate the minimum detection error\n",
    "    Y1 = ood_confidences\n",
    "    X1 = ind_confidences\n",
    "\n",
    "    start = np.min([np.min(X1), np.min(Y1)])\n",
    "    end = np.max([np.max(X1), np.max(Y1)])\n",
    "    gap = (end - start) / n_iter\n",
    "\n",
    "    best_error = 1.0\n",
    "    best_delta = None\n",
    "    all_thresholds = []\n",
    "    all_errors = []\n",
    "    for delta in np.arange(start, end, gap):\n",
    "        tpr = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
    "        error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
    "        detection_error = (tpr + error2) / 2.0\n",
    "\n",
    "        if return_data:\n",
    "            all_thresholds.append(delta)\n",
    "            all_errors.append(detection_error)\n",
    "\n",
    "        if detection_error < best_error:\n",
    "            best_error = np.minimum(best_error, detection_error)\n",
    "            best_delta = delta\n",
    "\n",
    "    if return_data:\n",
    "        return best_error, best_delta, all_errors, all_thresholds\n",
    "    else:\n",
    "        return best_error, best_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf9faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./pretrained_models', padding=True, truncation=True, return_tensors='pt', do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained('./pretrained_models', num_labels=config['K'])\n",
    "\n",
    "train_data = [{\"tgt_text\": example.tgt_text, \"label\": example.label} for example in dataset['train']]\n",
    "val_data = [{\"tgt_text\": example.tgt_text, \"label\": example.label} for example in dataset['val']]\n",
    "test_data = [{\"tgt_text\": example.tgt_text, \"label\": example.label} for example in dataset['test']]\n",
    "\n",
    "train_dataloader = create_data_loader(train_data, tokenizer, batch_size=config['batch_size'], max_length=max_seq_length)\n",
    "val_dataloader = create_test_data_loader(val_data, tokenizer, batch_size=config['batch_size'], max_length=max_seq_length)\n",
    "test_dataloader = create_test_data_loader(test_data, tokenizer, batch_size=config['batch_size'], max_length=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRODNet(nn.Module):\n",
    "    def __init__(self, backbone, feat_dim, num_classes):\n",
    "        super(GRODNet, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        if hasattr(self.backbone, 'fc'):\n",
    "            # remove fc otherwise ddp will\n",
    "            # report unused params\n",
    "            self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.lda = LDA(n_components=feat_dim)\n",
    "        self.pca = PCA(n_components=feat_dim)\n",
    "\n",
    "        self.n_cls = num_classes\n",
    "        self.head1 = nn.Linear(768, 2 * num_classes)\n",
    "        self.head = nn.Linear(768, self.n_cls + 1)\n",
    "        self.k = nn.Parameter(torch.tensor([0.1], dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "    def forward(self, x, y, attention): #x:data feature, y:label\n",
    "        feat = self.backbone.bert(x, attention)[1]#.squeeze() #(b,768)\n",
    "        # output = self.backbone(x)[0].squeeze() #(b,10)\n",
    "        self.lda.fit(feat, y)\n",
    "        X_lda = self.lda.transform(feat) #(b, feat_dim)\n",
    "        \n",
    "        self.pca.fit(feat)\n",
    "        X_pca = self.pca.transform(feat)\n",
    "\n",
    "        return feat, X_lda, X_pca\n",
    "    def intermediate_forward(self, x, attention):\n",
    "        feat = self.backbone.bert(x, attention)[1]\n",
    "        output = self.head(feat)\n",
    "        score = torch.softmax(output, dim=1)\n",
    "        score0 = output[:,:-1]\n",
    "        # score0 = torch.softmax(output[:,:-1], dim=1)\n",
    "        conf = torch.max(score, dim=1)\n",
    "        pred = torch.argmax(score, dim=1)\n",
    "        conf0 = torch.max(score0, dim=1)\n",
    "        pred0 = torch.argmax(score0, dim=1)\n",
    "        for i in range(pred.size(0)):\n",
    "            if pred[i] == output.size(1) - 1:\n",
    "                # conf[i] = 0.1\n",
    "                # pred[i] = 1\n",
    "                score0[i] = 0.1 * torch.ones(score0.size(1)).cuda() #1/K\n",
    "            # else:\n",
    "                # conf[i] = conf0[i]   \n",
    "        # return score0\n",
    "        return torch.softmax(score0, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class LDA(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        super(LDA, self).__init__()\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            n_samples, n_features = X.shape\n",
    "        except:\n",
    "            n_features = X.shape[0]\n",
    "        classes = torch.unique(y)\n",
    "        n_classes = len(classes)\n",
    "        \n",
    "        means = torch.zeros(n_classes, n_features).to(X.device)\n",
    "        for i, c in enumerate(classes):\n",
    "            try:\n",
    "                means[i] = torch.mean(X[y==c], dim=0)\n",
    "            except:\n",
    "                X = torch.unsqueeze(X, dim=0)\n",
    "                means[i] = torch.mean(X[y==c], dim=0)\n",
    "        \n",
    "        overall_mean = torch.mean(X, dim=0)\n",
    "        \n",
    "        within_class_scatter = torch.zeros(n_features, n_features).to(X.device)\n",
    "        for i, c in enumerate(classes):\n",
    "            class_samples = X[y==c]\n",
    "            deviation = class_samples - means[i]\n",
    "            within_class_scatter += torch.mm(deviation.t(), deviation)\n",
    "        \n",
    "        between_class_scatter = torch.zeros(n_features, n_features).to(X.device)\n",
    "        for i, c in enumerate(classes):\n",
    "            n = len(X[y==c])\n",
    "            mean_diff = (means[i] - overall_mean).unsqueeze(1)\n",
    "            between_class_scatter += n * torch.mm(mean_diff, mean_diff.t())\n",
    "\n",
    "        # torch.backends.cuda.preferred_linalg_library('magma')\n",
    "        # print((torch.inverse(within_class_scatter) @ between_class_scatter).size()) #(768,768)\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(\n",
    "        torch.inverse(within_class_scatter @ between_class_scatter  + 1e-2 * torch.eye((within_class_scatter @ between_class_scatter).size(0)).to(X.device)\n",
    "        ))\n",
    "        _, top_indices = torch.topk(eigenvalues, k=self.n_components, largest=True)\n",
    "        self.components = eigenvectors[:, top_indices]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return torch.mm(X, self.components)\n",
    "\n",
    "class PCA(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        super(PCA, self).__init__()\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        try:\n",
    "            n_samples, n_features = X.shape\n",
    "        except:\n",
    "            n_samples = 1\n",
    "        \n",
    "        self.mean = torch.mean(X, dim=0)\n",
    "        X_centered = X - self.mean\n",
    "        \n",
    "        covariance_matrix = torch.mm(X_centered.t(), X_centered) / max((n_samples - 1),1)\n",
    "        \n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(covariance_matrix)\n",
    "        _, top_indices = torch.topk(eigenvalues, k=self.n_components, largest=True)\n",
    "        self.components = eigenvectors[:, top_indices]\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return torch.mm(X_centered, self.components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import faiss.contrib.torch_utils\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from einops import repeat\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "class GRODTrainer:\n",
    "    def __init__(self, net: nn.Module, train_loader: DataLoader,\n",
    "                 config) -> None:\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = net\n",
    "        self.train_loader = train_loader\n",
    "        self.config = config\n",
    "\n",
    "        self.n_cls = config['dataset']['num_classes']\n",
    "\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            params=net.parameters(),\n",
    "            lr=config['optimizer']['lr'],\n",
    "            weight_decay=config['optimizer']['weight_decay'],\n",
    "        )\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max = 10\n",
    "        )\n",
    "\n",
    "        self.head = self.net.head\n",
    "        self.head1 = self.net.head1\n",
    "        self.alpha = config['trainer']['alpha']\n",
    "        self.nums_rounded = config['trainer']['nums_rounded']\n",
    "        self.gamma = config['trainer']['gamma']\n",
    "        \n",
    "        self.k = self.net.k\n",
    "        \n",
    "        self.best_accuracy = 0.0  # Update best accuracy\n",
    "        self.best_model_state = None  # Save current model state                \n",
    "                \n",
    "\n",
    "\n",
    "    def train(self, epochs):\n",
    "        # adjust_learning_rate(self.config, self.optimizer, epoch_idx - 1)\n",
    "        # self.net = nn.DataParallel(self.net)\n",
    "        self.net.train()\n",
    "        self.net.to(self.device)\n",
    "        for epoch_idx in range(epochs):\n",
    "            loss_avg = 0.0\n",
    "            train_dataiter = iter(self.train_loader)\n",
    "\n",
    "            for train_step in tqdm(range(1,\n",
    "                                        len(train_dataiter) + 1),\n",
    "                                desc='Epoch {:03d}: '.format(epoch_idx),\n",
    "                                position=0,\n",
    "                                leave=True,\n",
    "                                ):\n",
    "\n",
    "                batch = next(train_dataiter)\n",
    "                data = batch['input_ids'].to(self.device)\n",
    "                target = batch['label'].to(self.device)        \n",
    "                attention_mask = batch['attention_mask'].to(self.device)   \n",
    "\n",
    "                if data.size(0) / self.n_cls > 2:\n",
    "                    sub_datasets_in_mu = torch.zeros((self.n_cls, 768)).to(self.device) #(K,f)\n",
    "                    \n",
    "                    dataset_in_mu = torch.zeros(768).to(self.device) #(f)\n",
    "                    sub_datasets_in_cov = torch.zeros((self.n_cls, 768, 768)).to(self.device)\n",
    "                    sub_datasets_in_distances = torch.zeros(self.n_cls).to(self.device)\n",
    "                    data_in, feat_lda, feat_pca = self.net(data, target, attention_mask)\n",
    "                    \n",
    "                    # data_in = self.head1(data_in)\n",
    "                    data = data_in\n",
    "\n",
    "                    # generate rounded ood data\n",
    "                    sub_datasets_in = [Subset(data_in, torch.where(target == i)[0]) for i in range(self.n_cls)]\n",
    "                    sub_datasets_lda = [Subset(feat_lda, torch.where(target == i)[0]) for i in range(self.n_cls)]\n",
    "                    argmax = torch.zeros((self.n_cls, feat_lda.size()[1]))\n",
    "                    argmin = torch.zeros((self.n_cls, feat_lda.size()[1]))\n",
    "                    k = 0\n",
    "                    for i in range(self.n_cls):\n",
    "                        if len(sub_datasets_lda[i]) <= 1:\n",
    "                            pass\n",
    "                        else:\n",
    "                            k = k + 1\n",
    "                            dataloader = DataLoader(sub_datasets_lda[i], batch_size=64, shuffle=False)\n",
    "                            for batch in dataloader:\n",
    "                                tensor_data_lda = batch\n",
    "                            dataloader = DataLoader(sub_datasets_in[i], batch_size=64, shuffle=False)\n",
    "                            for batch in dataloader:\n",
    "                                tensor_data_in = batch\n",
    "                            argmax[i] = torch.argmax(tensor_data_lda,dim=0) #feat_dim\n",
    "                            argmax[i] = torch.argmin(tensor_data_lda,dim=0) #feat_dim\n",
    "\n",
    "                            for j in range(feat_lda.size()[1]):\n",
    "                                # print(argmax[i][j].item())\n",
    "                                if k == 1 and j==0:\n",
    "                                    data_rounded_category = tensor_data_in[int(argmax[i][j].item())].unsqueeze(0)\n",
    "                                    data_rounded_category_1 = tensor_data_in[int(argmin[i][j].item())].unsqueeze(0)\n",
    "                                else:\n",
    "                                    data_rounded_category = torch.cat((data_rounded_category, tensor_data_in[int(argmax[i][j].item())].unsqueeze(0)),dim=0)\n",
    "                                    data_rounded_category_1 = torch.cat((data_rounded_category_1, tensor_data_in[int(argmin[i][j].item())].unsqueeze(0)),dim=0)\n",
    "\n",
    "                            mean =  torch.mean(tensor_data_in, dim = 0)\n",
    "                            cov0 = self.calculate_covariance_matrix(tensor_data_in)+1e-4 * torch.eye(mean.size(0)).to(self.device)\n",
    "                            L = torch.linalg.cholesky(cov0)\n",
    "\n",
    "                            # 求解下三角矩阵 L 的逆\n",
    "                            L_inv = torch.linalg.inv(L)\n",
    "\n",
    "                            # 利用下三角矩阵的逆求解对称正定矩阵 A 的逆\n",
    "                            cov = torch.mm(L_inv.t(), L_inv)\n",
    "                            # cov = torch.linalg.inv(self.calculate_covariance_matrix(tensor_data_in)+1e-7 * torch.eye(mean.size(0)).to(self.device))\n",
    "                            # print(torch.mm(cov0, cov))\n",
    "                            if torch.max(torch.abs(sub_datasets_in_mu[i,:]))<1e-7:\n",
    "                                sub_datasets_in_cov[i,:,:] = cov\n",
    "                                sub_datasets_in_mu[i,:] = mean\n",
    "                                # print(self.mahalanobis(tensor_data_in, sub_datasets_in_mu, sub_datasets_in_cov))\n",
    "                                sub_datasets_in_distances[i] = torch.max(self.mahalanobis(tensor_data_in, sub_datasets_in_mu.clone(), sub_datasets_in_cov.clone())[:,i])                                                \n",
    "                                # print(\"1\",sub_datasets_in_distances[i])\n",
    "                            \n",
    "                            sub_datasets_in_cov[i,:,:] = 0.1 * cov.detach().clone().to(self.device) + 0.9 * sub_datasets_in_cov[i,:,:].detach().clone()\n",
    "                            # print(sub_datasets_in_cov)\n",
    "                            sub_datasets_in_mu[i,:] = 0.1 * mean.detach().clone().to(self.device) + 0.9 * sub_datasets_in_mu[i,:].detach().clone()\n",
    "                            dists = self.mahalanobis(tensor_data_in, sub_datasets_in_mu.clone(), sub_datasets_in_cov.clone())[:,i]\n",
    "                            dist = torch.max(dists)\n",
    "                            # print(dists.max(), dists.min())\n",
    "                            sub_datasets_in_distances[i] = 0.1 * dist.to(self.device).detach().clone() + 0.9 * sub_datasets_in_distances[i].detach().clone()\n",
    "                            # print(i, sub_datasets_in_distances[i])\n",
    "                            sub_datasets_in_mean =  repeat(sub_datasets_in_mu.clone()[i,:], \"f -> b f\", \n",
    "                                                        f = tensor_data_in.size(1), b = feat_lda.size()[1])\n",
    "                    \n",
    "                            A = data_rounded_category[-feat_lda.size()[1]:]\n",
    "                            A_1 = data_rounded_category_1[- feat_lda.size()[1]:]\n",
    "                            vector = F.normalize(A.to(self.device) - sub_datasets_in_mean.to(self.device), dim = 1)\n",
    "                            vector_1 = F.normalize(A_1.to(self.device) - sub_datasets_in_mean.to(self.device), dim = 1)\n",
    "                            A = A + self.alpha * vector.to(self.device) #(feat_dim, 768)\n",
    "                            A_1 = A_1 + self.alpha * vector_1.to(self.device) #(feat_dim, 768)\n",
    "                            if k == 1:\n",
    "                                mean_matrix_0 = A\n",
    "                                mean_matrix_1 = A_1\n",
    "                            else:\n",
    "                                mean_matrix_0 = torch.cat((mean_matrix_0, A), dim = 0) #(num, 768)\n",
    "                                mean_matrix_1 = torch.cat((mean_matrix_1, A_1), dim = 0) #(num, 768)\n",
    "                    mean_matrix = torch.cat((mean_matrix_0, mean_matrix_1), dim = 0)\n",
    "                    # print(mean_matrix.size())\n",
    "                    std = 1 / 3 * self.alpha\n",
    "                    mu = mean_matrix.T.unsqueeze(2).to(self.device) #(768,num,1)\n",
    "                    \n",
    "                    # print(mean_matrix.size())\n",
    "                    \n",
    "                    rand_data = torch.randn(768, int(self.nums_rounded)).to(self.device) #(768,nums_rounded)\n",
    "                    gaussian_data = mu + std * rand_data.unsqueeze(1) #(768, num, nums_rounded)\n",
    "                    # print(gaussian_data.size())\n",
    "                    nums = gaussian_data.size(1)\n",
    "                    nums_rounded = gaussian_data.size(2)\n",
    "                    reshaped_rounded_data = gaussian_data.permute(1, 2, 0).contiguous().view(nums * nums_rounded, mean_matrix.size(1)) # (num* nums_rounded, 768)\n",
    "                    data = torch.cat((data, reshaped_rounded_data), dim = 0)\n",
    "                    # print(reshaped_rounded_data.size())\n",
    "\n",
    "                    # generate PCA ood data\n",
    "                    argmax = torch.zeros(feat_pca.size()[1])\n",
    "                    argmax = torch.argmax(feat_pca,dim=0) #feat_dim\n",
    "                    \n",
    "                    argmin = torch.zeros(feat_pca.size()[1])\n",
    "                    argmin = torch.argmin(feat_pca,dim=0) #feat_dim\n",
    "\n",
    "                    for j in range(feat_pca.size()[1]):\n",
    "                        if j==0:\n",
    "                            pcadata_rounded_category = data_in[int(argmax[j].item())].unsqueeze(0)\n",
    "                            pcadata_rounded_category_1 = data_in[int(argmin[j].item())].unsqueeze(0)\n",
    "                        else:\n",
    "                            \n",
    "                            pcadata_rounded_category = torch.cat((pcadata_rounded_category, data_in[int(argmax[j].item())].unsqueeze(0)),dim=0)\n",
    "                            pcadata_rounded_category_1 = torch.cat((pcadata_rounded_category_1, data_in[int(argmin[j].item())].unsqueeze(0)),dim=0)\n",
    "                    if train_step == 1:\n",
    "                        dataset_in_mu = torch.mean(data_in, dim = 0)\n",
    "                    dataset_in_mu = 0.1 * torch.mean(data_in.detach().clone(), dim = 0) + 0.9 * dataset_in_mu.detach().clone() \n",
    "                    \n",
    "                    dataset_in_mu =  repeat(dataset_in_mu.squeeze(), \"f -> b f\", \n",
    "                                                    f = data_in.size(1), b = feat_lda.size()[1])\n",
    "                    # print(data_rounded_category.size())\n",
    "                    B = pcadata_rounded_category\n",
    "                    B_1 = pcadata_rounded_category_1\n",
    "                    # print(A.size())\n",
    "                    pcavector = F.normalize(B.clone() - dataset_in_mu, dim = 1)\n",
    "                    pcavector_1 = F.normalize(B_1.clone() - dataset_in_mu, dim = 1)\n",
    "                    B = torch.add(B, self.alpha * pcavector) #(feat_dim, 768)\n",
    "                    B_1 = torch.add(B_1, self.alpha * pcavector_1) #(feat_dim, 768)\n",
    "                    mean_matrix_0 = B\n",
    "                    mean_matrix_1 = B_1\n",
    "                    # print(A.size())\n",
    "                    mean_matrix = torch.cat((mean_matrix_0, mean_matrix_1), dim = 0)\n",
    "                    # mean_matrix = mean_matrix_0\n",
    "                    std = 1 / 3 * self.alpha\n",
    "                    mu = mean_matrix.T.unsqueeze(2).to(self.device) \n",
    "                    rand_data = torch.randn(mean_matrix.size(1), self.nums_rounded).to(self.device) \n",
    "                    gaussian_data = mu + std * rand_data.unsqueeze(1) #(768, num, nums_rounded)\n",
    "                    # print(gaussian_data.size())\n",
    "                    nums = gaussian_data.size(1)\n",
    "                    nums_rounded = gaussian_data.size(2)\n",
    "                    reshaped_rounded_data = gaussian_data.permute(1, 2, 0).contiguous().view(nums * nums_rounded, mean_matrix.size(1)) # (num* nums_rounded, 768)\n",
    "                    # print(reshaped_rounded_data.size(),data.size())\n",
    "                    data = torch.cat((data, reshaped_rounded_data), dim = 0)\n",
    "                    # target = torch.cat((target, (self.n_cls) * torch.ones(nums * nums_rounded).to(self.device)), dim = 0)\n",
    "                    # print(data.size())\n",
    "                    # filt id like data\n",
    "                    # print(sub_datasets_in_distances)\n",
    "                    data_add = data[data_in.size(0):]   \n",
    "                    # print(data_add.size())\n",
    "                    \n",
    "                    distances = self.mahalanobis(data_add, sub_datasets_in_mu, sub_datasets_in_cov).to(self.device) #(n,k)\n",
    "                    \n",
    "                    # print(distances, sub_datasets_in_distances)\n",
    "                    # 计算每个样本点的最小距离和对应的类别索引\n",
    "                    min_distances, min_distances_clas = torch.min(distances, dim=1)                       \n",
    "                    # 获取每个样本点对应的子数据集距离\n",
    "                    sub_distances = sub_datasets_in_distances[min_distances_clas.to(self.device)]\n",
    "                    \n",
    "                    k_init = (torch.mean(min_distances / sub_distances) - 1) * 10\n",
    "                    # 找到满足条件的索引\n",
    "                    mask = min_distances > (1 + k_init * self.k.to(self.device)[0]) * sub_distances\n",
    "                    \n",
    "                    \n",
    "                    # # 找到满足条件的索引\n",
    "                    # mask = min_distances > (1 + self.k.to(self.device)[0]) * sub_distances\n",
    "                    # print(min_distances,sub_distances)\n",
    "                    # 使用布尔索引删除满足条件的数据点\n",
    "                    cleaned_data_add = data_add[mask.to(self.device)]\n",
    "                \n",
    "                    if cleaned_data_add.size(0) > data_in.size(0) // self.n_cls + 2: #ood太多则随机删除\n",
    "                        delete_num = cleaned_data_add.size(0) - (data_in.size(0) // self.n_cls + 2)\n",
    "                        indices = torch.randperm(cleaned_data_add.size(0))[:(data_in.size(0) // self.n_cls + 2)].to(self.device)\n",
    "                        cleaned_data_add_de = cleaned_data_add[indices]\n",
    "                    else: \n",
    "                        cleaned_data_add_de = cleaned_data_add\n",
    "                        \n",
    "                    \n",
    "                    data = torch.cat((data[:data_in.size(0)], cleaned_data_add_de), dim = 0)\n",
    "\n",
    "\n",
    "                    target = torch.cat((target, (self.n_cls) * torch.ones(cleaned_data_add_de.size(0)).to(self.device)), dim = 0)\n",
    "                    \n",
    "                else:\n",
    "                    sub_datasets_in_mu = torch.zeros((self.n_cls, 768)).to(self.device) #(K,f)\n",
    "                    dataset_in_mu = torch.zeros(768).to(self.device) #(f)\n",
    "                    sub_datasets_in_cov = torch.zeros((self.n_cls, 768, 768)).to(self.device)\n",
    "                    sub_datasets_in_distances = torch.zeros(self.n_cls).to(self.device)\n",
    "                    data_in, feat_lda, feat_pca = self.net(data, target, attention_mask)\n",
    "                    # data_in = self.head1(data_in)\n",
    "                    data = data_in\n",
    "                    # generate PCA ood data\n",
    "                    argmax = torch.zeros(feat_pca.size()[1])\n",
    "                    argmax = torch.argmax(feat_pca,dim=0) #feat_dim\n",
    "                    \n",
    "                    argmin = torch.zeros(feat_pca.size()[1])\n",
    "                    argmin = torch.argmin(feat_pca,dim=0) #feat_dim\n",
    "\n",
    "                    for j in range(feat_pca.size()[1]):\n",
    "                        if j==0:\n",
    "                            pcadata_rounded_category = data_in[int(argmax[j].item())].unsqueeze(0)\n",
    "                            pcadata_rounded_category_1 = data_in[int(argmin[j].item())].unsqueeze(0)\n",
    "                        else:\n",
    "                            \n",
    "                            pcadata_rounded_category = torch.cat((pcadata_rounded_category, data_in[int(argmax[j].item())].unsqueeze(0)),dim=0)\n",
    "                            pcadata_rounded_category_1 = torch.cat((pcadata_rounded_category_1, data_in[int(argmin[j].item())].unsqueeze(0)),dim=0)\n",
    "                    if train_step == 1:\n",
    "                            dataset_in_mu = torch.mean(data_in, dim = 0)\n",
    "                    dataset_in_mu = 0.1 * torch.mean(data_in.detach().clone(), dim = 0) + 0.9 * dataset_in_mu.detach().clone() \n",
    "                    cov0 = self.calculate_covariance_matrix(data_in)+1e-4 * torch.eye(dataset_in_mu.size(0)).to(self.device)\n",
    "                    L = torch.linalg.cholesky(cov0)\n",
    "\n",
    "                    # 求解下三角矩阵 L 的逆\n",
    "                    L_inv = torch.linalg.inv(L)\n",
    "\n",
    "                    # 利用下三角矩阵的逆求解对称正定矩阵 A 的逆\n",
    "                    cov = torch.mm(L_inv.t(), L_inv)\n",
    "                    cov = torch.unsqueeze(cov, dim=0)\n",
    "                    dataset_in_mean =  repeat(dataset_in_mu.squeeze(), \"f -> b f\", \n",
    "                                                    f = data_in.size(1), b = feat_lda.size()[1])\n",
    "                    # dataset_in_mu = torch.unsqueeze(dataset_in_mu, dim=0)\n",
    "                    # print(data_rounded_category.size())\n",
    "                    B = pcadata_rounded_category\n",
    "                    B_1 = pcadata_rounded_category_1\n",
    "                    # print(A.size())\n",
    "                    pcavector = F.normalize(B.clone() - dataset_in_mean, dim = 1)\n",
    "                    pcavector_1 = F.normalize(B_1.clone() - dataset_in_mean, dim = 1)\n",
    "                    B = torch.add(B, self.alpha * pcavector) #(feat_dim, 768)\n",
    "                    B_1 = torch.add(B_1, self.alpha * pcavector_1) #(feat_dim, 768)\n",
    "                    mean_matrix_0 = B\n",
    "                    mean_matrix_1 = B_1\n",
    "                    # print(A.size())\n",
    "                    mean_matrix = torch.cat((mean_matrix_0, mean_matrix_1), dim = 0)\n",
    "                    # mean_matrix = mean_matrix_0\n",
    "                    std = 1 / 3 * self.alpha\n",
    "                    mu = mean_matrix.T.unsqueeze(2).to(self.device) \n",
    "                    rand_data = torch.randn(mean_matrix.size(1), self.nums_rounded).to(self.device) \n",
    "                    gaussian_data = mu + std * rand_data.unsqueeze(1) #(768, num, nums_rounded)\n",
    "                    # print(gaussian_data.size())\n",
    "                    nums = gaussian_data.size(1)\n",
    "                    nums_rounded = gaussian_data.size(2)\n",
    "                    reshaped_rounded_data = gaussian_data.permute(1, 2, 0).contiguous().view(nums * nums_rounded, mean_matrix.size(1)) # (num* nums_rounded, 768)\n",
    "                    # print(reshaped_rounded_data.size(),data.size())\n",
    "                    data = torch.cat((data, reshaped_rounded_data), dim = 0)\n",
    "                    # target = torch.cat((target, (self.n_cls) * torch.ones(nums * nums_rounded).to(self.device)), dim = 0)\n",
    "                    # print(data.size())\n",
    "                    # filt id like data\n",
    "                    # print(sub_datasets_in_distances)\n",
    "                    data_add = data[data_in.size(0):]   \n",
    "                    # print(data_add.size())\n",
    "                \n",
    "                    distances_add = self.mahalanobis(data_add, dataset_in_mu, cov).to(self.device).squeeze() #(n,1)\n",
    "                    distance = torch.max(self.mahalanobis(data[:data_in.size(0)], dataset_in_mu, cov).to(self.device))\n",
    "                    k_init = (torch.mean(distances_add) / distance - 1) * 10\n",
    "                    # 找到满足条件的索引\n",
    "                    mask = distances_add > (1 + k_init * self.k.to(self.device)[0]) * distance\n",
    "                    # print(distances_add, distance)\n",
    "                    \n",
    "                    # cleaned_data_add = data_add[mask.to(self.device)]\n",
    "                    cleaned_data_add = data_add\n",
    "                    \n",
    "                    if cleaned_data_add.size(0) > data_in.size(0) // self.n_cls + 2: #ood太多则随机删除\n",
    "                        delete_num = cleaned_data_add.size(0) - (data_in.size(0) // self.n_cls + 2)\n",
    "                        indices = torch.randperm(cleaned_data_add.size(0))[:(data_in.size(0) // self.n_cls + 2)].to(self.device)\n",
    "                        cleaned_data_add_de = cleaned_data_add[indices]\n",
    "                    else: \n",
    "                        cleaned_data_add_de = cleaned_data_add\n",
    "                    \n",
    "                \n",
    "                    data = torch.cat((data[:data_in.size(0)], cleaned_data_add_de), dim = 0)\n",
    "                    # data = data[:data_in.size(0)]\n",
    "\n",
    "\n",
    "                    target = torch.cat((target, (self.n_cls) * torch.ones(cleaned_data_add_de.size(0)).to(self.device)), dim = 0)\n",
    "                    # target = target\n",
    "                    \n",
    "\n",
    "                output = self.head(data)\n",
    "                output = F.normalize(output, dim=1)\n",
    "                loss1 = F.cross_entropy(output, target.to(torch.long))\n",
    "\n",
    "                label_matrix = output\n",
    "                biclas = torch.zeros(label_matrix.size(0), 2)\n",
    "                biclas[:,-1] = label_matrix[:,-1]\n",
    "                biclas[:,0] = torch.sum(label_matrix[:,:-1],-1)\n",
    "                label_biclas = torch.where(\n",
    "                    torch.gt(target, self.n_cls-0.5),                \n",
    "                    torch.ones(data.size()[0]).to(self.device),\n",
    "                    torch.zeros(data.size()[0]).to(self.device),\n",
    "                    )\n",
    "                loss2 = F.cross_entropy(biclas.to(self.device), label_biclas.to(torch.int64).to(self.device))\n",
    "\n",
    "                loss = (1 - self.gamma) * loss1 + self.gamma * loss2\n",
    "                \n",
    "                # backward\n",
    "                self.optimizer.zero_grad()\n",
    "                # print(loss1,loss2)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "                # exponential moving average, show smooth values\n",
    "                with torch.no_grad():\n",
    "                    loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "            print(f'Epoch {epoch_idx + 1}/{epochs}, Average Training Loss: {loss_avg:.4f}')\n",
    "            # accuracy = correct / total\n",
    "            # print(f'Accuracy on validation set: {accuracy:.4f}')\n",
    "            accuracy = self.test_model()  # Test model after each epoch\n",
    "            if accuracy > self.best_accuracy or accuracy == self.best_accuracy:  # If current accuracy is better than best\n",
    "                self.best_accuracy = accuracy  # Update best accuracy\n",
    "                self.best_model_state = self.net.state_dict()  # Save current model state                \n",
    "                \n",
    "    def test_model(self):\n",
    "        self.net.eval()  # Switch to evaluation mode\n",
    "        self.net.to(self.device)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_dataiter = iter(self.train_loader)\n",
    "        with torch.no_grad():\n",
    "            for train_step in tqdm(range(1, len(val_dataiter) + 1),\n",
    "                                    position=0,\n",
    "                                    leave=True):\n",
    "                batch = next(val_dataiter)\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                data_in, feat_lda, feat_pca = self.net(input_ids, labels, attention_mask)\n",
    "                outputs = self.head(data_in)\n",
    "                # outputs = self.net.backbone(input_ids, attention_mask).logits\n",
    "                predicted = torch.argmax(outputs[:,:-1], dim=1)\n",
    "                # print(predicted, labels)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "        print(f'Accuracy on validation set: {accuracy:.4f}')\n",
    "        return accuracy    \n",
    "\n",
    "    def save_best_model(self, filename):\n",
    "        if self.best_model_state is not None:\n",
    "            torch.save(self.best_model_state, filename)\n",
    "\n",
    "    \n",
    "    def mahalanobis(self, x, support_mean, inv_covmat): #(n,d), (k,d), (k,d,d)\n",
    "        # 获取输入的张量维度信息\n",
    "        n = x.size(0)\n",
    "        d = x.size(1)\n",
    "\n",
    "        # 将输入张量和支持向量均值移到 GPU 上\n",
    "        x = x.cuda()\n",
    "        support_mean = support_mean.cuda()\n",
    "\n",
    "        maha_dists = []\n",
    "        for i in range(inv_covmat.size(0)):\n",
    "            class_inv_cov = inv_covmat[i].detach()\n",
    "            support_class = support_mean[i].detach()\n",
    "        \n",
    "            x_mu = x - support_class.unsqueeze(0).expand(n, d)            \n",
    "            # 将类别协方差矩阵移到 GPU 上\n",
    "            class_inv_cov = class_inv_cov.cuda()\n",
    "            # print(torch.max(torch.abs(class_inv_cov)))\n",
    "\n",
    "            # 计算 Mahalanobis 距离\n",
    "            left = torch.matmul(x_mu, class_inv_cov)\n",
    "            # print(left.size())\n",
    "            mahal = torch.matmul(left, x_mu.t()).diagonal()\n",
    "            # print('2', mahal) \n",
    "            # print(torch.matmul(left, x_mu.t()))\n",
    "            maha_dists.append(mahal)\n",
    "\n",
    "        # 将结果转换为 PyTorch 张量\n",
    "        return torch.stack(maha_dists).t()\n",
    "    \n",
    "    def calculate_covariance_matrix(self, data):\n",
    "        # 计算数据的均值\n",
    "        mean = torch.mean(data, dim=0)\n",
    "        mean = mean.unsqueeze(0).expand(data.size(0), data.size(1))\n",
    "        # 将数据减去均值，得到去中心化的数据\n",
    "        centered_data = data - mean\n",
    "\n",
    "        # 计算协方差矩阵\n",
    "        covariance_matrix = torch.mm(centered_data.t(), centered_data) / (centered_data.size(0) - 1 + 1e-7)\n",
    "        # print(covariance_matrix.size())\n",
    "        return covariance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = 'grod.yml'\n",
    "loaded_parameters = load_parameters_from_yaml(yaml_file_path)\n",
    "config_grod = loaded_parameters\n",
    "        \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_grod = GRODNet(model, 1, 2) #(model, num, K)\n",
    "trainer = GRODTrainer(model_grod, train_dataloader, config_grod)\n",
    "trainer.train(config_grod['optimizer']['num_epochs']) \n",
    "\n",
    "# Save the best model state\n",
    "trainer.save_best_model('best_model_grod.ckpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56256d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载保存的状态字典\n",
    "state_dict = torch.load('best_model_grod.ckpt')\n",
    "model_grod = GRODNet(model, 1, 2)\n",
    "# 将加载的状态字典加载到模型中\n",
    "model_grod.load_state_dict(state_dict)\n",
    "print(model_grod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f342109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from numpy.linalg import norm, pinv\n",
    "from scipy.special import logsumexp\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from tqdm import tqdm\n",
    "\n",
    "# run\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class BasePostprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def setup(self, net: nn.Module, id_loader_dict, ood_loader_dict):\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def postprocess(self, net: nn.Module, data: Any, attention):\n",
    "        output = net(input_ids=data, attention_mask=attention)\n",
    "        score = torch.softmax(output.logits, dim=1)\n",
    "        conf, pred = torch.max(score, dim=1)\n",
    "        return pred, conf\n",
    "\n",
    "    def inference(self,\n",
    "                  net: nn.Module,\n",
    "                  data_loader: DataLoader, \n",
    "                  alpha, w, b, u, NS,\n",
    "                  progress: bool = True):\n",
    "        pred_list, conf_list, label_list = [], [], []\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].cuda()\n",
    "            attention_mask = batch[\"attention_mask\"].cuda()\n",
    "            labels = batch['label'].cuda()\n",
    "            pred, conf = self.postprocess(net.cuda(), input_ids, attention_mask, alpha, w, b, u, NS,)\n",
    "\n",
    "            pred_list.append(pred.cpu())\n",
    "            conf_list.append(conf.cpu())\n",
    "            label_list.append(labels.cpu())\n",
    "\n",
    "        # convert values into numpy array\n",
    "        pred_list = torch.cat(pred_list).numpy().astype(int)\n",
    "        conf_list = torch.cat(conf_list).numpy()\n",
    "        label_list = torch.cat(label_list).numpy().astype(int)\n",
    "\n",
    "        return pred_list, conf_list, label_list\n",
    "\n",
    "\n",
    "class GRODPostprocessor(BasePostprocessor):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.args = self.config['postprocessor']['postprocessor_args']\n",
    "        self.args_dict = self.config['postprocessor']['postprocessor_sweep']\n",
    "        self.dim = self.args['dim']\n",
    "        self.setup_flag = False\n",
    "\n",
    "    def setup(self, net: nn.Module, id_loader_dict):\n",
    "        if not self.setup_flag:\n",
    "            net.eval()\n",
    "            net.cuda()\n",
    "            with torch.no_grad():\n",
    "                # self.w, self.b = net.backbone.get_fc()\n",
    "                # print(self.b.size)\n",
    "                self.w, self.b = net.head.weight[:-1,:].cpu().numpy(), net.head.bias[:-1].cpu().numpy()\n",
    "                # print(self.w.size())\n",
    "                print('Extracting id training feature')\n",
    "                feature_id_train = []\n",
    "                logit_id_train = []\n",
    "                for batch in tqdm(id_loader_dict,\n",
    "                                  desc='Setup: ',\n",
    "                                  position=0,\n",
    "                                  leave=True):\n",
    "                    data = batch['input_ids'].cuda()\n",
    "                    attention_mask = batch[\"attention_mask\"].cuda()\n",
    "                    labels = batch['label'].cuda()\n",
    "                    # data = data.float()\n",
    "                    feature = net.backbone.bert(data, attention_mask)[1]\n",
    "                    logit = net.head(feature)\n",
    "                    score = torch.softmax(logit, dim=1)\n",
    "                    score0 = torch.softmax(logit[:,:-1], dim=1)\n",
    "                    conf, pred = torch.max(score, dim=1)\n",
    "                    conf0, pred0 = torch.max(score0, dim=1)\n",
    "                    for i in range(pred.size(0)):\n",
    "                        if pred[i] == logit.size(1) - 1:\n",
    "                            conf[i] = 0.1\n",
    "                            pred[i] = 1\n",
    "                            score0[i, :] = 0.1 * torch.ones(score0.size(1)).cuda()\n",
    "                        else:\n",
    "                            conf[i] = conf0[i]     \n",
    "                        \n",
    "                    feature_id_train.append(feature.cpu().numpy())\n",
    "                    logit_id_train.append(score0.cpu().numpy())\n",
    "                feature_id_train = np.concatenate(feature_id_train, axis=0)\n",
    "                logit_id_train = np.concatenate(logit_id_train, axis=0)\n",
    "\n",
    "                # logit_id_train = feature_id_train @ self.w.T + self.b\n",
    "\n",
    "            self.u = -np.matmul(pinv(self.w), self.b)\n",
    "            ec = EmpiricalCovariance(assume_centered=True)\n",
    "            ec.fit(feature_id_train - self.u)\n",
    "            eig_vals, eigen_vectors = np.linalg.eig(ec.covariance_)\n",
    "            self.NS = np.ascontiguousarray(\n",
    "                (eigen_vectors.T[np.argsort(eig_vals * -1)[self.dim:]]).T)\n",
    "\n",
    "            vlogit_id_train = norm(np.matmul(feature_id_train - self.u,\n",
    "                                             self.NS),\n",
    "                                   axis=-1)\n",
    "            \n",
    "            print(feature_id_train - self.u, self.NS)\n",
    "            \n",
    "            self.alpha = logit_id_train.max(\n",
    "                axis=-1).mean() / vlogit_id_train.mean()\n",
    "            print(f'{self.alpha=:.4f}')\n",
    "\n",
    "            self.setup_flag = True\n",
    "        else:\n",
    "            pass\n",
    "        return self.alpha, self.w, self.b, self.u, self.NS\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def postprocess(self, net: nn.Module, data: Any, attention, alpha, w, b, u, NS):\n",
    "        feature_ood = net.backbone.bert(data, attention)[1]\n",
    "        \n",
    "        logit = net.head(feature_ood)\n",
    "        score = torch.softmax(logit, dim=1)\n",
    "        score0 = torch.softmax(logit[:,:-1], dim=1)\n",
    "        conf, pred = torch.max(score, dim=1)\n",
    "        conf0, pred0 = torch.max(score0, dim=1)\n",
    "        for i in range(pred.size(0)):\n",
    "          if pred[i] == logit.size(1) - 1:\n",
    "            conf[i] = 0.1\n",
    "            pred[i] = 1\n",
    "            score0[i, :] = 0.1 * torch.ones(score0.size(1)).cuda()\n",
    "          else:\n",
    "            conf[i] = conf0[i]\n",
    "        logit_ood = score0.cpu()    \n",
    "        \n",
    "        feature_ood = feature_ood.cpu()\n",
    "        \n",
    "        # logit_ood = feature_ood @ w.T + b\n",
    "        _, pred = torch.max(logit_ood, dim=1)\n",
    "        energy_ood = logsumexp(logit_ood.numpy(), axis=-1)\n",
    "        vlogit_ood = norm(np.matmul(feature_ood.numpy() - u, NS),\n",
    "                          axis=-1) * alpha\n",
    "        score_ood = -vlogit_ood + energy_ood\n",
    "        return pred, torch.from_numpy(score_ood)\n",
    "\n",
    "    def set_hyperparam(self, hyperparam: list):\n",
    "        self.dim = hyperparam[0]\n",
    "\n",
    "    def get_hyperparam(self):\n",
    "        return self.dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = 'grod.yml'\n",
    "loaded_parameters = load_parameters_from_yaml(yaml_file_path)\n",
    "config_grod = loaded_parameters\n",
    "alpha, w, b, u, NS = GRODPostprocessor(config_grod).setup(model_grod, train_dataloader)\n",
    "\n",
    "pred_list, conf_list, label_list = GRODPostprocessor(config_grod).inference(model_grod, test_dataloader, alpha, w, b, u, NS)\n",
    "\n",
    "# [fpr, auroc, aupr_in, aupr_out, accuracy]\n",
    "compute_all_metrics(conf_list, label_list, pred_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bfdd80c4cd5b3ca30f79c8858286326028dc154b9efddfc3ea147df9fc4c063"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ood': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
